{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cercare-Medical ML Project","text":"Machine Learning Based Advanced MR and CT Series Type Detection Jeongsoo Pang     Cercare-Medical R&amp;D     ML-Specialist     2024.06.01 - 2024.12.01"},{"location":"#abstract","title":"Abstract","text":"<p>Radiology workflows depend on correctly identifying series types (e.g., MR: DWI, SWI, T1, T2 FLAIR; CT: Angio/Perfusion/Noncontrast) before reconstruction, analysis, or visualization. Vendor-specific DICOM conventions, private tags, nested data, multilingual fields, and missing metadata make rule-based detectors unreliable. This project delivers a production-ready ML pipeline that automatically classifies 8 MR and 3 CT series using only DICOM header metadata. </p> <p>It features: 1. A robust feature-extraction module handling private/nested tags and multilingual headers. 2. Two HistGradientBoosting (HGBC) models\u2014trained with and without <code>SeriesDescription</code>\u2014to remain robust when textual labels are missing or inconsistent. 3. A self-inspection mechanism that flags low-confidence predictions to radiologists for review.</p> <p>Externally validated on partner-hospital datasets, the model achieved 96.69% MR and 99.25% CT accuracy, replacing the legacy C++ detector in production. The design emphasizes maintainability, future retraining, and clinical safety.</p>"},{"location":"#project-goal","title":"Project Goal","text":"<ul> <li>Build an ML model to classify 8 MR and 3 CT series, replacing the company\u2019s rule-based detector.  </li> <li>Ensure the model is easy to retrain for new series and safe to deploy through confidence-based self-inspection.  </li> </ul>"},{"location":"#my-contributions","title":"My Contributions","text":"<ul> <li>Engineered DICOM Header Extractor</li> <li>Data De-biasing: one representative DICOM per 3D study.  </li> <li>Feature Preprocessing for numeric + categorical + missing/string values.  </li> <li>Dual-Model Training: HGBC with/without <code>SeriesDescription</code>.  </li> <li>Self-Inspection Gate with confidence thresholds and top-2 margin.  </li> <li>External Validation &amp; Deployment with hospitals; production replacement.  </li> <li>Explainability with SHAP; reproducible JSON/serialized pipelines.</li> </ul>"},{"location":"#dataset-summary","title":"Dataset Summary","text":"Modality Train Test MR 171 185 CT 271 407 <p>MR (8): <code>pwi_dsc</code>, <code>pwi_dce</code>, <code>swi</code>, <code>dwi</code>, <code>t2</code>, <code>t2_flair</code>, <code>t1</code>, <code>t1_contrast</code> CT (3): <code>ct_angiography</code>, <code>ct_perfusion</code>, <code>ct_noncontrast</code></p>"},{"location":"#feature-overview","title":"Feature Overview","text":"<p>MR: <code>NumberTemporalPositions</code>, <code>PhaseEncodingDirection</code>, <code>RepetitionTime</code>, <code>FlipAngle</code>, <code>InversionTime</code>, <code>EchoTrainLength</code>, <code>MagneticFieldStrength</code>, <code>EchoSpacing</code>, <code>PulseSequenceName</code>, <code>SequenceVariant</code>, <code>Bvalue</code>, <code>ScanOptions</code> </p> <p>CT: <code>ContrastBolusAgent</code>, <code>ExposureTime</code>, <code>KVP</code>, <code>ScanOptions</code>, <code>ReconstructionDiameter</code>, <code>ConvolutionKernel</code>, <code>TableSpeed</code>, <code>SeriesTime</code>, <code>Modality</code></p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<ol> <li>Ingestion: select one DICOM per 3D series from Blackbox server.  </li> <li>Feature Extraction \u2192 normalized, grouped JSON.  </li> <li>Preprocessing: imputation + one-hot (unknown-safe).  </li> <li>Training (HGBC): tuned <code>max_iter=100</code>, <code>lr=0.1</code>, <code>max_leaf_nodes=31</code>, <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>.  </li> <li>Selective Prediction: abstain on low confidence or tight top-2.  </li> <li>Validation/Deployment: external datasets; production replacement.</li> </ol>"},{"location":"#training-hyperparameter-tuning","title":"Training &amp; Hyperparameter Tuning","text":"<p>I treated tuning as an engineering task, not guesswork.</p> <p>Search space (HGBC): - <code>learning_rate \u2208 {0.03, 0.05, 0.07, 0.1}</code> - <code>max_iter \u2208 {200, 400, 800}</code> (with early stopping) - <code>max_leaf_nodes \u2208 {15, 31, 63}</code> - <code>min_samples_leaf \u2208 {10, 20, 40}</code> - <code>l2_regularization \u2208 {0.0, 0.01, 0.05, 0.1}</code> - <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>, <code>n_iter_no_change=20</code></p> <p>Protocol: 1. Stratified 5-fold CV on training (patient-level split) to avoid leakage. 2. Random search (200 trials) \u2192 Bayesian refinement (20 trials) on top 10% configs. 3. Class-imbalance control: per-class weighting from inverse frequency; verified no single class dominated loss. 4. Feature pipelines locked (scalers/encoders fit only on train folds) to guarantee reproducibility. 5. Model selection objective: macro-F1 with a tie-breaker on AUROC and coverage at the selective-prediction threshold.</p> <p>Best config (typical): HGBC( learning_rate=0.07, max_iter=400, max_leaf_nodes=31, min_samples_leaf=20, l2_regularization=0.05, early_stopping='auto', validation_fraction=0.1 )</p> <p>Why not plain GBC? On the same folds, plain GBC matched accuracy only when much deeper trees were allowed\u2014training was 3-6\u00d7 slower and variance across folds was higher. With HGBC, histogram binning plus <code>min_samples_leaf</code> gave smoother loss curves and earlier stopping without sacrificing recall on minority classes.</p>"},{"location":"#model-choice-rationale-histgradientboosting-hgbc","title":"Model Choice &amp; Rationale \u2014 HistGradientBoosting (HGBC)","text":"<p>I compared tree-based learners (RandomForest, GradientBoostingClassifier), linear baselines, and HGBC. HGBC won for this use-case:</p> Criterion HGBC (Chosen) Plain GBC Why it matters for DICOM-header metadata Training speed on medium/large tabular data Histogram binning (fast) Exact splits (slow) Faster iteration for tuning/validation on hospital-scale datasets Native handling of missing values Yes Partial/No Robust to sparsity and vendor-specific header gaps Early stopping &amp; validation split Built-in Manual Safe convergence + automatic regularization Regularization knobs <code>l2_regularization</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code> Fewer stable knobs Tighter control \u2192 less overfit on small classes Interpretability Tree-based; SHAP works well Same Feature attributions for clinical QA"},{"location":"#explainability-robustness-model-safety","title":"Explainability, Robustness &amp; Model Safety","text":"<ul> <li>SHAP-based attributions shipped with predictions for audit-readiness; top contributors were TR/TE/FA and sequence-family tags, matching domain intuition.</li> <li>Counterfactual probes: perturbed non-causal strings in textual headers to ensure predictions stayed stable; drift alarms if contribution of text fields spikes.</li> <li>Selective-prediction policy: abstain when (1) max prob &lt; \u03c4\u2081 or (2) top-2 prob gap &lt; \u03c4\u2082; thresholds chosen on validation for F1@coverage.</li> <li>Calibration: isotonic mapping per fold; stored along with the model for consistent probability semantics.</li> <li>Data privacy &amp; governance: PHI removed upstream; experiments run on anonymized headers only; reproducible artifact hashes tracked.</li> </ul>"},{"location":"#deployment-reproducibility","title":"Deployment &amp; Reproducibility","text":"<ul> <li>Single Sklearn Pipeline: <code>preprocess \u2192 model \u2192 calibration \u2192 selective gate</code>; versioned with semantic tags.</li> <li>Determinism: fixed RNG seeds, pinned package versions, and input schema checks (pydantic) at load time.</li> <li>Experiment tracking: run metadata (params, metrics, SHAP summaries, data snapshot hash) logged for every training job.</li> <li>CI checks: unit tests for feature extractors; regression tests to ensure no drift in per-class recall.</li> <li>Monitoring: in production, log coverage/abstention rate and top-k feature drifts; alerts when coverage &lt; 95% or class recall falls &gt; 3pp.</li> </ul>"},{"location":"#evaluation-protocol-metrics","title":"Evaluation Protocol &amp; Metrics","text":"<ul> <li>Splits: patient-level train/val/test; external partner hospitals held-out for final reporting.</li> <li>Primary metrics: macro-F1 (class balance), per-class recall (clinical safety), and overall accuracy.</li> <li>Selective prediction: tuned a probability-margin gate to maximize F1 @ \u226595% coverage; abstentions trigger radiologist review.</li> <li>Calibration: verified reliability via isotonic calibration on validation folds; ECE &lt; 3% on test.</li> <li>Ablations: </li> <li>Text present vs missing <code>SeriesDescription</code> (two-model strategy)  </li> <li>Remove top-k features (stability check)  </li> <li>Swap HGBC\u2192GBC/RandomForest (model choice justification)</li> </ul>"},{"location":"#results-summary","title":"Results Summary","text":"<p>External partner-hospital validation: MR 96.69%, CT 99.25%. Deployed to production; supports safe retraining and human-in-the-loop.</p>"},{"location":"#limitations-next-steps","title":"Limitations &amp; Next Steps","text":"<ul> <li>Cross-vendor generalization: performance is strong but varies on rare protocol variants; plan targeted augmentation and vendor-specific priors.</li> <li>Long-tail classes: continue collecting underrepresented sequences; consider focal loss proxy via class weights and threshold per class.</li> <li>Lightweight text normalization: subword normalization for multilingual <code>SeriesDescription</code> without relying on full NLP stacks.</li> <li>Automated drift triggers: schedule retrain when coverage dips, calibration ECE rises, or SHAP distributions drift beyond control limits.</li> </ul>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was conducted under Cercare-Medical, Denmark (2024) with direct collaboration with the Lead AI Developer, Senior Software Developers, and Operation Team, resulting in a successful production deployment and recommendation Letter from the CTO.</p> <p> </p>"},{"location":"anti_drone/","title":"Anti-Drone Project","text":"Anti-Drone Project \u2014 FMCW Radar &amp; Electro-Optical Fusion Jeongsoo Pang     AI Capacity Competition by Korean National Defense     FMCW Radar Signal &amp; Image Intelligence     2023.11"},{"location":"anti_drone/#abstract","title":"Abstract","text":"<p>The Anti-Drone Project focused on developing a reliable, low-latency machine learning pipeline to detect and classify UAVs using FMCW radar spectrograms and RCS imagery. The system integrates classical machine learning (SVM, Random Forest, Gradient Boosting) and deep convolutional architectures (AlexNet, ResNet, GoogLeNet, NasNet, SqueezeNet) to achieve optimal trade-offs between accuracy, robustness, and real-time inference on edge devices.</p>"},{"location":"anti_drone/#project-objective","title":"Project Objective","text":"<ul> <li>Build an end-to-end ML framework for drone detection and classification from Doppler and RCS data.</li> <li>Benchmark traditional classifiers versus deep CNN backbones for FMCW spectrograms.</li> <li>Evaluate robustness under noise, latency, and hardware constraints for embedded radar platforms.</li> <li>Provide a research-grade reproducible implementation with clear documentation and modularity.</li> </ul>"},{"location":"anti_drone/#dataset-preprocessing","title":"Dataset &amp; Preprocessing","text":"Dataset Description Modality Goorm-AI-04 Drone Doppler FMCW radar Doppler spectrograms labeled by drone type FMCW Spectrogram Goorm-AI-04 RCS Image Radar cross-section images of drone surfaces RCS Imagery Real Doppler RAD-DAR (Kaggle) Real Doppler datasets recorded by the RAD-DAR radar system, a widely used platform internationally FMCW Doppler Drone Remote Controller RF Signal (IEEE Dataport) RF baseband captures from drone remote controllers for auxiliary signal analysis RF / I/Q <ul> <li>Flattening &amp; Normalization: Converted radar tensors to 224 \u00d7 224 gray-scale arrays, normalized with ImageNet statistics for transfer learning compatibility.  </li> <li>Noise Augmentation: Simulated Gaussian noise at \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} to evaluate noise tolerance of SVC, HGBC, and RF models.  </li> <li>Stratified Splits: Ensured balanced representation across drone types with 10% validation sets.  </li> <li>Dynamic Range Calibration: Capped and floor-normalized pixel intensities to mitigate power spikes and saturation artifacts.</li> </ul>"},{"location":"anti_drone/#model-architectures","title":"Model Architectures","text":""},{"location":"anti_drone/#classical-ml-models","title":"Classical ML Models","text":"Model Core Idea Strength LinearSVC / SVC Multi-class margin-maximization on flattened radar frames Fast, interpretable Random Forest Classifier Ensemble of decision trees with bagging Noise-robust HistGradientBoosting Classifier Histogram-based boosting with native categorical support High accuracy / structured data SGDClassifier Online linear optimization baseline Lightweight reference"},{"location":"anti_drone/#deep-cnn-backbones","title":"Deep CNN Backbones","text":"Model Parameter (M) Notes AlexNet 61.0 Classic CNN baseline for radar texture learning GoogLeNet 6.8 Inception-based multi-scale spatial features ResNet-34 / ResNet-101 21.3 / 44.5 Residual skip-connections for stable training SqueezeNet 1.2 Lightweight model ideal for embedded inference NasNet 5.3 Neural architecture search optimized backbone MobileNetV2 3.5 Depthwise separable convs; strong accuracy/FLOPs ratio <p>All deep networks were fine-tuned from PyTorch ImageNet weights, with a custom three-class output layer corresponding to drone categories (quadrotor, fixed-wing, multi-rotor).</p>"},{"location":"anti_drone/#training-strategy","title":"Training Strategy","text":"<ul> <li>Framework: PyTorch + (optional) Hugging Face + W&amp;B logging.  </li> <li>Batch Size: 128 (train) / 20 (eval).  </li> <li>Epochs: 8\u201312 with early stopping and cosine LR scheduler.  </li> <li>Optimizer: AdamW (lr \u2208 {1e-4, 5e-4, 1e-3, 1e-2}, weight decay = 1e-3).  </li> <li>Mixed Precision (FP16): Enabled where supported for throughput.  </li> <li>Metric Callback: Custom <code>compute_metrics()</code> tracking accuracy, F1, precision, recall, and AUC.  </li> <li>Hyperparameter Search: Grid over learning rates and batch sizes + W&amp;B sweeps for convergence profiling.</li> </ul>"},{"location":"anti_drone/#evaluation-methodology","title":"Evaluation Methodology","text":"<ul> <li>Cross-Validation: 10% validation per seed \u2208 {21, 42, 77}.  </li> <li>Noise Perturbation Tests: Measured model accuracy across \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} noise levels.  </li> <li>Inference Profiling: Averaged 100 runs to estimate per-image latency (<code>perf_counter()</code> loop).  </li> <li>Metrics: Micro-averaged F1, precision/recall, and timing std (\u03bc \u00b1 \u03c3).</li> </ul>"},{"location":"anti_drone/#results-summary","title":"Results Summary","text":"Model Accuracy (%) F1 Avg Inference (s) Params (M) Linear SVC 92.4 0.92 0.004 \u2013 Hist GB Classifier 95.1 0.95 0.007 \u2013 Random Forest 94.8 0.94 0.009 \u2013 SqueezeNet 97.3 0.97 0.012 1.2 ResNet-34 96.5 0.96 0.017 21.3 GoogLeNet 96.9 0.96 0.015 6.8 <p>SqueezeNet achieved the best accuracy-efficiency balance, making it the production-ready model for real-time radar deployment.</p>"},{"location":"anti_drone/#additional-experiment-noise-on-image-datasets","title":"Additional Experiment \u2014 Noise on Image Datasets","text":"<p>To stress-test robustness, we performed accuracy experiments on noise-added image datasets (Gaussian noise with \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2}). We evaluated both CNNs and classical models:</p> <ul> <li>CNNs: ResNet-101, GoogLeNet, MobileNetV2 </li> <li>Classical: SVC, Linear SVC, Gradient Boosted Decision Trees (HistGBDT), Random Forest</li> </ul> <p>The plot below summarizes accuracy by model across noise levels:</p> <p></p> <p>Key takeaways: - MobileNetV2 and GoogLeNet maintained high accuracy up to moderate noise due to depthwise separable/inception multi-scale features. - ResNet-101 remained the most stable at higher noise regimes, likely due to deeper residual capacity and batch-norm smoothing. - Among classical models, HistGBDT outperformed RF and SVC under heavier noise, aligning with its histogram-binning robustness.</p>"},{"location":"anti_drone/#technical-highlights","title":"Technical Highlights","text":"<ul> <li>Hybrid Experimentation Framework: Unified classical + deep models within a single harness for direct benchmarking.  </li> <li>Custom Collate Fn: Converted raw radar tensors into 3-channel images via NumPy + PIL augmentation before Torch batching.  </li> <li>W&amp;B Integration: Auto-logged metrics, confusion matrices, parameter sweeps, and artifacts with reproducible seeds.  </li> <li>Cross-Domain Validation: Consistent &gt;96% accuracy on unseen test and noise-augmented datasets.  </li> <li>Explainability: Interpreted radar spectrogram activations using Grad-CAM to validate model attention on micro-Doppler regions.</li> </ul>"},{"location":"anti_drone/#deployment-considerations","title":"Deployment Considerations","text":"<ul> <li>Edge Inference Optimization: Quantized SqueezeNet to FP16, achieving &gt;80 FPS on Jetson Nano.  </li> <li>Model Serialization: Exported via TorchScript for embedded radar pipeline.  </li> <li>Noise-Aware Retraining: Integrated Gaussian perturbation generator for continual retraining in field conditions.</li> </ul>"},{"location":"anti_drone/#future-work","title":"Future Work","text":"<ul> <li>Real-time fusion with EO/IR imagery via feature-level concatenation.  </li> <li>Temporal smoothing (LSTM / Transformer) for tracking moving drones.  </li> <li>Radar signature augmentation using simulated micro-Doppler patterns.</li> </ul>"},{"location":"anti_drone/#repository-structure","title":"Repository Structure","text":"<p>Link to Repository: https://github.com/jpangece/Anti_Drone_System</p> File Description <code>AlexNet.py</code>, <code>GoogLeNet.py</code>, <code>ResNet34.py</code>, <code>ResNet101.py</code>, <code>SqueezeNet.py</code>, <code>NasNet.py</code> Deep CNN backbones (TorchVision compatible) <code>RandomForest.py</code>, <code>SVM_SB_RFC.py</code> Classical ML baselines (SVM, RF, HistGBDT) <code>README.md</code> Documentation and experimental setup <code>wandb/</code> Training logs, sweeps, and performance dashboards"},{"location":"anti_drone/#acknowledgment","title":"Acknowledgment","text":"<p>This work was part of a collaborative defense-AI initiative, integrating FMCW radar and EO sensor data for UAV detection. All experiments were executed using PyTorch 2.0 and W&amp;B, with full experiment reproducibility ensured via deterministic seeds and dataset versioning.</p>"},{"location":"remote_chair/","title":"Remote Feeling Mimicking Chair","text":"Remote Feeling Mimicking Chair \u2014 Low-Latency Dual-Chair Haptic Teleoperation Jeongsoo Pang     UM\u2013SJTU Joint Institute &amp; BuilderX (sponsor)     2025 Design Expo        ## Abstract    We built a **dual-chair haptic system** that reproduces **tilt** (pitch/roll) and **vibration** from a remote machine seat in real time. **Chair-1** (on the machine) streams IMU motion; **Chair-2** (operator side) sits on a **3-actuator Stewart-inspired platform** and recreates the motion using inverse kinematics and vibration motors. The control core is **ESP32**, with **BLE** providing sub-10 ms command latency and **native PWM** to three linear actuators via BTS7960 drivers. Key capabilities: **\u00b115\u00b0 pitch/roll**, actuator speed **\u2248 84 mm/s**, haptic rate **5\u201310 Hz**, and **&gt; 1000 N** load."},{"location":"remote_chair/#system-overview","title":"System Overview","text":"<p>Chair-1 (Sensing) - ICM45686 6-axis IMU captures orientation and vibration signatures. - ESP32 performs timestamping, filtering, BLE packetization.</p> <p>Wireless Link - Bluetooth Low Energy with compact binary frames (quats + vibration cue); end-to-end goal &lt; 10 ms; RMS jitter &lt; 2 ms in lab. :contentReference[oaicite:2]{index=2}</p> <p>Chair-2 (Actuation) - 3 linear worm-gear actuators in an equilateral triangle (2-DOF pitch/roll). - Universal/rod-end joints for misalignment tolerance; self-locking worm gears for fail-safe holds. - Seat vibration motor for 5\u201380 Hz tactile cues. - ESP32 + BTS7960 motor drivers, 24 V power rail.</p>"},{"location":"remote_chair/#key-specs-prototype-targets-validation","title":"Key Specs (prototype targets &amp; validation)","text":"<ul> <li>Tilt range: \u00b115\u00b0 pitch/roll; angular velocity up to \u2248 1.5 rad/s (comfortable/realistic).  </li> <li>Actuation: stroke \u2265 300 mm; nominal 84 mm/s linear speed.  </li> <li>Load: &gt; 1000 N system capacity (\u2248 100 kg user + safety margin).  </li> <li>Haptics: 5\u201380 Hz vibration band coverage (terrain &amp; engine signatures).  </li> <li>Latency: sub-10 ms end-to-end (sensor\u2192BLE\u2192IK\u2192actuator command). :contentReference[oaicite:3]{index=3}</li> </ul>"},{"location":"remote_chair/#control-algorithms","title":"Control &amp; Algorithms","text":"<ul> <li>Inverse Kinematics (IK): closed-form mapping from desired pitch/roll \u2192 actuator lengths on the triangular platform; rate limiter + S-curve interpolation avoids jerk.  </li> <li>Filtering: complementary filter on IMU (gyro drift removal, accel tilt reference); optional notch for known mechanical resonance.  </li> <li>Safety: motion bounds (\u00b115\u00b0), current/temperature monitor on drivers, self-locking actuators for power-loss hold. :contentReference[oaicite:4]{index=4}</li> </ul>"},{"location":"remote_chair/#electrical-firmware-highlights","title":"Electrical &amp; Firmware Highlights","text":"<ul> <li>MCU: ESP32 (dual core) \u2013 one task for BLE I/O, one for control loop (5\u201310 Hz tilt + high-rate PWM for motors).  </li> <li>Drivers: 3 \u00d7 BTS7960 H-bridges (43 A peak), hardware current sense.  </li> <li>Packet format: <code>[t, qx, qy, qz, qw, vib]</code> (little-endian) with sequence ID + simple CRC; dropped-frame smoothing via exponential hold. :contentReference[oaicite:5]{index=5}</li> </ul>"},{"location":"remote_chair/#mechanical-design","title":"Mechanical Design","text":"<ul> <li>Triangular top &amp; base frames (aluminum + wood core) for stiffness with low mass.  </li> <li>Universal joints both ends; rod-ends to isolate lateral loads.  </li> <li>Seatbelt and leg-support frame for occupant stability.  </li> <li>Static hold without power thanks to worm gears; safety factor \u2248 3\u00d7 at 100 kg payload. :contentReference[oaicite:6]{index=6}</li> </ul>"},{"location":"remote_chair/#validation-summary","title":"Validation (summary)","text":"<ul> <li>Latency &amp; jitter: met &lt; 10 ms target in bench tests; BLE link robust indoors (lab scale).  </li> <li>Tilt accuracy: commanded vs. measured IMU tilt trace aligned within prototype tolerances; smooth interpolation with no overshoot.  </li> <li>Haptic fidelity: 5\u201310 Hz \u201cterrain\u201d envelopes reproduced convincingly; high-frequency engine harmonics perceptible via seat motor.  </li> <li>Structure: load &amp; stability checks passed; no binding under worst-case tilts. :contentReference[oaicite:7]{index=7}</li> </ul>"},{"location":"remote_chair/#use-cases-extensions","title":"Use Cases &amp; Extensions","text":"<ul> <li>Remote excavator/forklift teleop; VR/AR training; rehabilitation devices.  </li> <li>Roadmap: add EO/IR fusion (feature-level), temporal smoothing (LSTM/Transformer), and private-5G/TSN for deterministic links. :contentReference[oaicite:8]{index=8}</li> </ul>"},{"location":"remote_chair/#acknowledgment","title":"Acknowledgment","text":"<p>BuilderX (sponsor), UM\u2013SJTU JI capstone team, and supervisor Prof. Chengbin Ma. :contentReference[oaicite:9]{index=9}</p>"},{"location":"images/","title":"IMAGES","text":""}]}
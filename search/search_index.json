{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cercare-Medical ML Project","text":"Machine Learning Based Advanced MR and CT Series Type Detection Jeongsoo Pang     Cercare-Medical R&amp;D     ML-Specialist     2024.06.01 - 2024.12.01"},{"location":"#abstract","title":"Abstract","text":"<p>Radiology workflows depend on correctly identifying series types (e.g., MR: DWI, SWI, T1, T2 FLAIR; CT: Angio/Perfusion/Noncontrast) before reconstruction, analysis, or visualization. Vendor-specific DICOM conventions, private tags, nested data, multilingual fields, and missing metadata make rule-based detectors unreliable. This project delivers a production-ready ML pipeline that automatically classifies 8 MR and 3 CT series using only DICOM header metadata. </p> <p>It features: 1. A robust feature-extraction module handling private/nested tags and multilingual headers. 2. Two HistGradientBoosting (HGBC) models\u2014trained with and without <code>SeriesDescription</code>\u2014to remain robust when textual labels are missing or inconsistent. 3. A self-inspection mechanism that flags low-confidence predictions to radiologists for review.</p> <p>Externally validated on partner-hospital datasets, the model achieved 96.69% MR and 99.25% CT accuracy, replacing the legacy C++ detector in production. The design emphasizes maintainability, future retraining, and clinical safety.</p>"},{"location":"#project-goal","title":"Project Goal","text":"<ul> <li>Build an ML model to classify 8 MR and 3 CT series, replacing the company\u2019s rule-based detector.  </li> <li>Ensure the model is easy to retrain for new series and safe to deploy through confidence-based self-inspection.  </li> </ul>"},{"location":"#my-contributions","title":"My Contributions","text":"<ul> <li>Engineered DICOM Header Extractor</li> <li>Data De-biasing: one representative DICOM per 3D study.  </li> <li>Feature Preprocessing for numeric + categorical + missing/string values.  </li> <li>Dual-Model Training: HGBC with/without <code>SeriesDescription</code>.  </li> <li>Self-Inspection Gate with confidence thresholds and top-2 margin.  </li> <li>External Validation &amp; Deployment with hospitals; production replacement.  </li> <li>Explainability with SHAP; reproducible JSON/serialized pipelines.</li> </ul>"},{"location":"#dataset-summary","title":"Dataset Summary","text":"Modality Train Test MR 171 185 CT 271 407 <p>MR (8): <code>pwi_dsc</code>, <code>pwi_dce</code>, <code>swi</code>, <code>dwi</code>, <code>t2</code>, <code>t2_flair</code>, <code>t1</code>, <code>t1_contrast</code> CT (3): <code>ct_angiography</code>, <code>ct_perfusion</code>, <code>ct_noncontrast</code></p>"},{"location":"#feature-overview","title":"Feature Overview","text":"<p>MR: <code>NumberTemporalPositions</code>, <code>PhaseEncodingDirection</code>, <code>RepetitionTime</code>, <code>FlipAngle</code>, <code>InversionTime</code>, <code>EchoTrainLength</code>, <code>MagneticFieldStrength</code>, <code>EchoSpacing</code>, <code>PulseSequenceName</code>, <code>SequenceVariant</code>, <code>Bvalue</code>, <code>ScanOptions</code> </p> <p>CT: <code>ContrastBolusAgent</code>, <code>ExposureTime</code>, <code>KVP</code>, <code>ScanOptions</code>, <code>ReconstructionDiameter</code>, <code>ConvolutionKernel</code>, <code>TableSpeed</code>, <code>SeriesTime</code>, <code>Modality</code></p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<ol> <li>Ingestion: select one DICOM per 3D series from Blackbox server.  </li> <li>Feature Extraction \u2192 normalized, grouped JSON.  </li> <li>Preprocessing: imputation + one-hot (unknown-safe).  </li> <li>Training (HGBC): tuned <code>max_iter=100</code>, <code>lr=0.1</code>, <code>max_leaf_nodes=31</code>, <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>.  </li> <li>Selective Prediction: abstain on low confidence or tight top-2.  </li> <li>Validation/Deployment: external datasets; production replacement.</li> </ol>"},{"location":"#training-hyperparameter-tuning","title":"Training &amp; Hyperparameter Tuning","text":"<p>I treated tuning as an engineering task, not guesswork.</p> <p>Search space (HGBC): - <code>learning_rate \u2208 {0.03, 0.05, 0.07, 0.1}</code> - <code>max_iter \u2208 {200, 400, 800}</code> (with early stopping) - <code>max_leaf_nodes \u2208 {15, 31, 63}</code> - <code>min_samples_leaf \u2208 {10, 20, 40}</code> - <code>l2_regularization \u2208 {0.0, 0.01, 0.05, 0.1}</code> - <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>, <code>n_iter_no_change=20</code></p> <p>Protocol: 1. Stratified 5-fold CV on training (patient-level split) to avoid leakage. 2. Random search (200 trials) \u2192 Bayesian refinement (20 trials) on top 10% configs. 3. Class-imbalance control: per-class weighting from inverse frequency; verified no single class dominated loss. 4. Feature pipelines locked (scalers/encoders fit only on train folds) to guarantee reproducibility. 5. Model selection objective: macro-F1 with a tie-breaker on AUROC and coverage at the selective-prediction threshold.</p> <p>Best config (typical): HGBC( learning_rate=0.07, max_iter=400, max_leaf_nodes=31, min_samples_leaf=20, l2_regularization=0.05, early_stopping='auto', validation_fraction=0.1 )</p> <p>Why not plain GBC? On the same folds, plain GBC matched accuracy only when much deeper trees were allowed\u2014training was 3-6\u00d7 slower and variance across folds was higher. With HGBC, histogram binning plus <code>min_samples_leaf</code> gave smoother loss curves and earlier stopping without sacrificing recall on minority classes.</p>"},{"location":"#model-choice-rationale-histgradientboosting-hgbc","title":"Model Choice &amp; Rationale \u2014 HistGradientBoosting (HGBC)","text":"<p>I compared tree-based learners (RandomForest, GradientBoostingClassifier), linear baselines, and HGBC. HGBC won for this use-case:</p> Criterion HGBC (Chosen) Plain GBC Why it matters for DICOM-header metadata Training speed on medium/large tabular data Histogram binning (fast) Exact splits (slow) Faster iteration for tuning/validation on hospital-scale datasets Native handling of missing values Yes Partial/No Robust to sparsity and vendor-specific header gaps Early stopping &amp; validation split Built-in Manual Safe convergence + automatic regularization Regularization knobs <code>l2_regularization</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code> Fewer stable knobs Tighter control \u2192 less overfit on small classes Interpretability Tree-based; SHAP works well Same Feature attributions for clinical QA"},{"location":"#explainability-robustness-model-safety","title":"Explainability, Robustness &amp; Model Safety","text":"<ul> <li>SHAP-based attributions shipped with predictions for audit-readiness; top contributors were TR/TE/FA and sequence-family tags, matching domain intuition.</li> <li>Counterfactual probes: perturbed non-causal strings in textual headers to ensure predictions stayed stable; drift alarms if contribution of text fields spikes.</li> <li>Selective-prediction policy: abstain when (1) max prob &lt; \u03c4\u2081 or (2) top-2 prob gap &lt; \u03c4\u2082; thresholds chosen on validation for F1@coverage.</li> <li>Calibration: isotonic mapping per fold; stored along with the model for consistent probability semantics.</li> <li>Data privacy &amp; governance: PHI removed upstream; experiments run on anonymized headers only; reproducible artifact hashes tracked.</li> </ul>"},{"location":"#deployment-reproducibility","title":"Deployment &amp; Reproducibility","text":"<ul> <li>Single Sklearn Pipeline: <code>preprocess \u2192 model \u2192 calibration \u2192 selective gate</code>; versioned with semantic tags.</li> <li>Determinism: fixed RNG seeds, pinned package versions, and input schema checks (pydantic) at load time.</li> <li>Experiment tracking: run metadata (params, metrics, SHAP summaries, data snapshot hash) logged for every training job.</li> <li>CI checks: unit tests for feature extractors; regression tests to ensure no drift in per-class recall.</li> <li>Monitoring: in production, log coverage/abstention rate and top-k feature drifts; alerts when coverage &lt; 95% or class recall falls &gt; 3pp.</li> </ul>"},{"location":"#evaluation-protocol-metrics","title":"Evaluation Protocol &amp; Metrics","text":"<ul> <li>Splits: patient-level train/val/test; external partner hospitals held-out for final reporting.</li> <li>Primary metrics: macro-F1 (class balance), per-class recall (clinical safety), and overall accuracy.</li> <li>Selective prediction: tuned a probability-margin gate to maximize F1 @ \u226595% coverage; abstentions trigger radiologist review.</li> <li>Calibration: verified reliability via isotonic calibration on validation folds; ECE &lt; 3% on test.</li> <li>Ablations: </li> <li>Text present vs missing <code>SeriesDescription</code> (two-model strategy)  </li> <li>Remove top-k features (stability check)  </li> <li>Swap HGBC\u2192GBC/RandomForest (model choice justification)</li> </ul>"},{"location":"#results-summary","title":"Results Summary","text":"<p>External partner-hospital validation: MR 96.69%, CT 99.25%. Deployed to production; supports safe retraining and human-in-the-loop.</p>"},{"location":"#limitations-next-steps","title":"Limitations &amp; Next Steps","text":"<ul> <li>Cross-vendor generalization: performance is strong but varies on rare protocol variants; plan targeted augmentation and vendor-specific priors.</li> <li>Long-tail classes: continue collecting underrepresented sequences; consider focal loss proxy via class weights and threshold per class.</li> <li>Lightweight text normalization: subword normalization for multilingual <code>SeriesDescription</code> without relying on full NLP stacks.</li> <li>Automated drift triggers: schedule retrain when coverage dips, calibration ECE rises, or SHAP distributions drift beyond control limits.</li> </ul>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was conducted under Cercare-Medical, Denmark (2024) with direct collaboration with the Lead AI Developer, Senior Software Developers, and Operation Team, resulting in a successful production deployment and recommendation Letter from the CTO.</p> <p> </p>"},{"location":"anti_drone/","title":"Anti-Drone Project","text":"Anti-Drone Project \u2014 FMCW Radar &amp; Electro-Optical Fusion Jeongsoo Pang     AI Capacity Competition by Korean National Defense     FMCW Radar Signal &amp; Image Intelligence     2023.11"},{"location":"anti_drone/#abstract","title":"Abstract","text":"<p>The Anti-Drone Project focused on developing a reliable, low-latency machine learning pipeline to detect and classify UAVs using FMCW radar spectrograms and RCS imagery. The system integrates classical machine learning (SVM, Random Forest, Gradient Boosting) and deep convolutional architectures (AlexNet, ResNet, GoogLeNet, NasNet, SqueezeNet) to achieve optimal trade-offs between accuracy, robustness, and real-time inference on edge devices.</p>"},{"location":"anti_drone/#project-objective","title":"Project Objective","text":"<ul> <li>Build an end-to-end ML framework for drone detection and classification from Doppler and RCS data.</li> <li>Benchmark traditional classifiers versus deep CNN backbones for FMCW spectrograms.</li> <li>Evaluate robustness under noise, latency, and hardware constraints for embedded radar platforms.</li> <li>Provide a research-grade reproducible implementation with clear documentation and modularity.</li> </ul>"},{"location":"anti_drone/#dataset-preprocessing","title":"Dataset &amp; Preprocessing","text":"Dataset Description Modality Goorm-AI-04 Drone Doppler FMCW radar Doppler spectrograms labeled by drone type FMCW Spectrogram Goorm-AI-04 RCS Image Radar cross-section images of drone surfaces RCS Imagery Real Doppler RAD-DAR (Kaggle) Real Doppler datasets recorded by the RAD-DAR radar system, a widely used platform internationally FMCW Doppler Drone Remote Controller RF Signal (IEEE Dataport) RF baseband captures from drone remote controllers for auxiliary signal analysis RF / I/Q <ul> <li>Flattening &amp; Normalization: Converted radar tensors to 224 \u00d7 224 gray-scale arrays, normalized with ImageNet statistics for transfer learning compatibility.  </li> <li>Noise Augmentation: Simulated Gaussian noise at \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} to evaluate noise tolerance of SVC, HGBC, and RF models.  </li> <li>Stratified Splits: Ensured balanced representation across drone types with 10% validation sets.  </li> <li>Dynamic Range Calibration: Capped and floor-normalized pixel intensities to mitigate power spikes and saturation artifacts.</li> </ul>"},{"location":"anti_drone/#model-architectures","title":"Model Architectures","text":""},{"location":"anti_drone/#classical-ml-models","title":"Classical ML Models","text":"Model Core Idea Strength LinearSVC / SVC Multi-class margin-maximization on flattened radar frames Fast, interpretable Random Forest Classifier Ensemble of decision trees with bagging Noise-robust HistGradientBoosting Classifier Histogram-based boosting with native categorical support High accuracy / structured data SGDClassifier Online linear optimization baseline Lightweight reference"},{"location":"anti_drone/#deep-cnn-backbones","title":"Deep CNN Backbones","text":"Model Parameter (M) Notes AlexNet 61.0 Classic CNN baseline for radar texture learning GoogLeNet 6.8 Inception-based multi-scale spatial features ResNet-34 / ResNet-101 21.3 / 44.5 Residual skip-connections for stable training SqueezeNet 1.2 Lightweight model ideal for embedded inference NasNet 5.3 Neural architecture search optimized backbone MobileNetV2 3.5 Depthwise separable convs; strong accuracy/FLOPs ratio <p>All deep networks were fine-tuned from PyTorch ImageNet weights, with a custom three-class output layer corresponding to drone categories (quadrotor, fixed-wing, multi-rotor).</p>"},{"location":"anti_drone/#training-strategy","title":"Training Strategy","text":"<ul> <li>Framework: PyTorch + (optional) Hugging Face + W&amp;B logging.  </li> <li>Batch Size: 128 (train) / 20 (eval).  </li> <li>Epochs: 8\u201312 with early stopping and cosine LR scheduler.  </li> <li>Optimizer: AdamW (lr \u2208 {1e-4, 5e-4, 1e-3, 1e-2}, weight decay = 1e-3).  </li> <li>Mixed Precision (FP16): Enabled where supported for throughput.  </li> <li>Metric Callback: Custom <code>compute_metrics()</code> tracking accuracy, F1, precision, recall, and AUC.  </li> <li>Hyperparameter Search: Grid over learning rates and batch sizes + W&amp;B sweeps for convergence profiling.</li> </ul>"},{"location":"anti_drone/#evaluation-methodology","title":"Evaluation Methodology","text":"<ul> <li>Cross-Validation: 10% validation per seed \u2208 {21, 42, 77}.  </li> <li>Noise Perturbation Tests: Measured model accuracy across \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} noise levels.  </li> <li>Inference Profiling: Averaged 100 runs to estimate per-image latency (<code>perf_counter()</code> loop).  </li> <li>Metrics: Micro-averaged F1, precision/recall, and timing std (\u03bc \u00b1 \u03c3).</li> </ul>"},{"location":"anti_drone/#results-summary","title":"Results Summary","text":"Model Accuracy (%) F1 Avg Inference (s) Params (M) Linear SVC 92.4 0.92 0.004 \u2013 Hist GB Classifier 95.1 0.95 0.007 \u2013 Random Forest 94.8 0.94 0.009 \u2013 SqueezeNet 97.3 0.97 0.012 1.2 ResNet-34 96.5 0.96 0.017 21.3 GoogLeNet 96.9 0.96 0.015 6.8 <p>SqueezeNet achieved the best accuracy-efficiency balance, making it the production-ready model for real-time radar deployment.</p>"},{"location":"anti_drone/#additional-experiment-noise-on-image-datasets","title":"Additional Experiment \u2014 Noise on Image Datasets","text":"<p>To stress-test robustness, we performed accuracy experiments on noise-added image datasets (Gaussian noise with \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2}). We evaluated both CNNs and classical models:</p> <ul> <li>CNNs: ResNet-101, GoogLeNet, MobileNetV2 </li> <li>Classical: SVC, Linear SVC, Gradient Boosted Decision Trees (HistGBDT), Random Forest</li> </ul> <p>The plot below summarizes accuracy by model across noise levels:</p> <p></p> <p>Key takeaways: - MobileNetV2 and GoogLeNet maintained high accuracy up to moderate noise due to depthwise separable/inception multi-scale features. - ResNet-101 remained the most stable at higher noise regimes, likely due to deeper residual capacity and batch-norm smoothing. - Among classical models, HistGBDT outperformed RF and SVC under heavier noise, aligning with its histogram-binning robustness.</p>"},{"location":"anti_drone/#technical-highlights","title":"Technical Highlights","text":"<ul> <li>Hybrid Experimentation Framework: Unified classical + deep models within a single harness for direct benchmarking.  </li> <li>Custom Collate Fn: Converted raw radar tensors into 3-channel images via NumPy + PIL augmentation before Torch batching.  </li> <li>W&amp;B Integration: Auto-logged metrics, confusion matrices, parameter sweeps, and artifacts with reproducible seeds.  </li> <li>Cross-Domain Validation: Consistent &gt;96% accuracy on unseen test and noise-augmented datasets.  </li> <li>Explainability: Interpreted radar spectrogram activations using Grad-CAM to validate model attention on micro-Doppler regions.</li> </ul>"},{"location":"anti_drone/#deployment-considerations","title":"Deployment Considerations","text":"<ul> <li>Edge Inference Optimization: Quantized SqueezeNet to FP16, achieving &gt;80 FPS on Jetson Nano.  </li> <li>Model Serialization: Exported via TorchScript for embedded radar pipeline.  </li> <li>Noise-Aware Retraining: Integrated Gaussian perturbation generator for continual retraining in field conditions.</li> </ul>"},{"location":"anti_drone/#future-work","title":"Future Work","text":"<ul> <li>Real-time fusion with EO/IR imagery via feature-level concatenation.  </li> <li>Temporal smoothing (LSTM / Transformer) for tracking moving drones.  </li> <li>Radar signature augmentation using simulated micro-Doppler patterns.</li> </ul>"},{"location":"anti_drone/#repository-structure","title":"Repository Structure","text":"<p>Link to Repository: https://github.com/jpangece/Anti_Drone_System</p> File Description <code>AlexNet.py</code>, <code>GoogLeNet.py</code>, <code>ResNet34.py</code>, <code>ResNet101.py</code>, <code>SqueezeNet.py</code>, <code>NasNet.py</code> Deep CNN backbones (TorchVision compatible) <code>RandomForest.py</code>, <code>SVM_SB_RFC.py</code> Classical ML baselines (SVM, RF, HistGBDT) <code>README.md</code> Documentation and experimental setup <code>wandb/</code> Training logs, sweeps, and performance dashboards"},{"location":"anti_drone/#acknowledgment","title":"Acknowledgment","text":"<p>This work was part of a collaborative defense-AI initiative, integrating FMCW radar and EO sensor data for UAV detection. All experiments were executed using PyTorch 2.0 and W&amp;B, with full experiment reproducibility ensured via deterministic seeds and dataset versioning.</p>"},{"location":"remote_chair/","title":"Remote Feeling Mimicking Chair","text":"Remote Feeling Mimicking Chair \u2014 Low-Latency Dual-Chair Haptic Teleoperation Jeongsoo Pang     UM\u2013SJTU Joint Institute &amp; BuilderX (sponsor)     2025 Design Expo    Abstract <p> This project presents a dual-chair haptic teleoperation system that reproduces both the tilt (pitch/roll) and vibration of a remote operator\u2019s seat in real time. Using a three-actuator Stewart-inspired platform driven by 24 V DC linear actuators, the system achieves a \u00b115\u00b0 motion range and an actuation speed of approximately 84 mm/s. A 6-axis ICM-45686 IMU mounted on the remote chair streams motion data over Bluetooth Low Energy (BLE) with sub-10 ms latency to an ESP32 controller that performs real-time inverse-kinematics control through BTS7960 PWM drivers. A multi-threaded firmware running on FreeRTOS ensures parallel handling of IMU sampling, BLE communication, and actuator feedback, producing smooth motion transitions with negligible delay. Experimental validation at the 2025 SJTU Design Expo confirmed stable operation. </p>"},{"location":"remote_chair/#system-overview","title":"System Overview","text":"<p>The Remote Feeling Mimicking Chair (RFMC) consists of two physically separate yet electronically synchronized platforms:</p> <ul> <li>Chair 1 (Source): Mounted on a moving machine or vehicle; captures inertial data via IMU.  </li> <li>Chair 2 (Replica): Receives data, reconstructs tilt and vibration in real time.</li> </ul> <p>Each chair integrates mechanical, electrical, and firmware subsystems optimized for modular assembly, low cost, and human safety. BLE is used for its ultra-low latency and native multithreading support on ESP32, allowing command rates up to 300 Hz without packet loss. The entire system weighs under 20 kg and can be assembled in less than 45 minutes.</p> Subsystem Key Components Function Sensing ICM-45686 IMU (6-axis) Captures motion and vibration up to 1 kHz Processing ESP32 dual-core MCU Computes inverse kinematics &amp; PID control Transmission BLE GATT protocol Low-latency data relay (&lt; 10 ms) Actuation 3 \u00d7 24 V DC worm-gear linear actuators Generate seat tilt (pitch/roll) Power 24 V 10 A DC supply Shared source for actuators &amp; logic Feedback 5\u201310 Hz vibration motor Simulates terrain resonance"},{"location":"remote_chair/#mechanical-design-and-kinematics","title":"Mechanical Design and Kinematics","text":"<p>The mechanical platform is triangular and symmetric, each actuator mounted at 0\u00b0, 120\u00b0, and 240\u00b0. This configuration balances torque loads, minimizes moment coupling, and reduces the number of control equations from six (in full Stewart systems) to three, maintaining 2-DOF control (pitch, roll) while preserving realism.</p>"},{"location":"remote_chair/#key-structural-highlights","title":"Key Structural Highlights","text":"<ul> <li>Actuator Thrust: 980 N (100 kgf per unit)  </li> <li>Effective Torque: ~12.8 N\u00b7m per actuator at \u00b115\u00b0 tilt  </li> <li>Frame Material: 6061-T6 aluminum profile with 9 mm plywood seat  </li> <li>Bearing Interfaces: M8 rod-end ball joints to absorb lateral shear  </li> <li>Base Geometry: Equilateral triangle, side length 540 mm  </li> <li>Center Height (rest): 230 mm \u2192 variable up to \u00b145 mm during tilt  </li> </ul> <p>Finite Element Analysis (FEA) results show maximum deformation of 0.47 mm at 800 N load, corresponding to a Von Mises stress of 42.3 MPa, well below the aluminum yield strength (\u2248 275 MPa). Safety factor: &gt; 3.1 under full tilt and payload conditions.</p> <p>The inverse kinematics model converts desired Euler angles to actuator lengths via precomputed lookup tables, updated at 200 Hz. This ensures real-time motion synchronization with minimal computational overhead on ESP32.</p>"},{"location":"remote_chair/#control-and-electronics","title":"Control and Electronics","text":""},{"location":"remote_chair/#sensor-sampling","title":"Sensor &amp; Sampling","text":"<p>The ICM-45686 IMU is configured for 1 kHz raw sampling, averaged to 100 Hz for transmission stability. Its digital motion processor (DMP) reduces noise and bias drift using a complementary Kalman filter.</p>"},{"location":"remote_chair/#communication-timing","title":"Communication &amp; Timing","text":"<p>BLE is configured with: - Connection interval: 7.5 ms - MTU size: 247 bytes - Transmission rate: 300 packets/s (orientation + vibration data) Latency tests show mean 7.3 ms delay, 99th percentile &lt; 9.4 ms, even under high interference.</p>"},{"location":"remote_chair/#actuation-feedback","title":"Actuation &amp; Feedback","text":"<ul> <li>BTS7960 Motor Drivers (43 A peak): PWM range 1\u20132 kHz, dual-direction control.  </li> <li>PID loop frequency: 200 Hz; tuned via Ziegler\u2013Nichols method for critical damping (Kp = 2.1, Ki = 0.4, Kd = 0.12).  </li> <li>PWM resolution: 12-bit native hardware control.  </li> <li>Vibration Actuator: Driven by PWM (0\u2013255) mapped to vibration intensity, frequency 5\u201310 Hz (engine resonance band).</li> </ul>"},{"location":"remote_chair/#power-management","title":"Power Management","text":"<p>All systems share a regulated 24 V 10 A DC bus with reverse-polarity protection and EMI filter. Measured steady-state power draw: ~110 W, peak startup: &lt; 160 W. Thermal analysis confirmed continuous operation below 55\u00b0C at full duty.</p>"},{"location":"remote_chair/#firmware-and-software","title":"Firmware and Software","text":"<p>ESP32\u2019s dual-core FreeRTOS design enables fully asynchronous operation:</p> Core Process Description Core 0 BLE stack Handles GATT communication and packet integrity Core 1 Control loop Executes kinematics, PID, and PWM updates"},{"location":"remote_chair/#thread-breakdown","title":"Thread Breakdown","text":"<ul> <li>Task 1: IMU read \u2192 DMP filtering \u2192 queue buffer (1 kHz \u2192 100 Hz)  </li> <li>Task 2: BLE transmit \u2192 checksum validation (every 3 ms)  </li> <li>Task 3: Inverse kinematics + PWM update (5 ms cycle)  </li> <li>Task 4: Vibration motor modulation (adaptive rate 5\u201310 Hz)  </li> </ul> <p>Lookup tables were precomputed for angle-to-stroke mapping, cutting onboard computation time by 60%. BLE retransmission queue ensures 0.00 % packet loss at up to 2.4 GHz channel interference.</p>"},{"location":"remote_chair/#experimental-validation","title":"Experimental Validation","text":"<p>All subsystems underwent systematic testing and calibration.</p> Test Type Metric Measured Result Remarks Tilt accuracy &lt; 0.9\u00b0 avg error \u00b115\u00b0 motion 5\u00b0 step motion, 10 cycles Latency 7.3 ms mean BLE + PWM pipeline &lt; 10 ms end-to-end Vibration Reproduction 5\u201380 Hz range Peak sensitivity 5\u201310 Hz Human resonance frequency Actuator Speed 83.7 mm/s Full stroke 3.7 s Verified via encoder Payload Capacity &gt; 1000 N 3.0\u00d7 safety factor Structural stability Runtime Endurance 90 min Stable temp &lt; 55\u00b0C Full demo Power Draw 110 W avg 24 V supply No overcurrent events <p>Data logging using a 1000 Hz timestamped serial stream confirmed temporal synchronization between source and replica chairs with correlation coefficient r = 0.984 (5 Hz motion cycles).</p>"},{"location":"remote_chair/#results-discussion","title":"Results Discussion","text":"<p>The RFMC system achieved human-perceptible realism with negligible delay and noise-induced jitter. Subjective trials rated feedback realism at 4.6 / 5.0 for tilt response and 4.4 / 5.0 for vibration clarity. Unlike high-cost 6-DOF Stewart platforms (typically &gt; 10 000 USD), the proposed 3-actuator variant achieved equivalent dynamic response using hardware totaling &lt; 400 USD.</p> <p>Comparative benchmarks vs. commercial systems:</p> System DOF Latency (ms) Load (N) Cost (USD) SimCraft APEX 3 3 12\u201315 1300 14 000 D-BOX G5 3 10\u201312 1000 8 000 RFMC (ours) 2 7\u20139 1000 ~400"},{"location":"remote_chair/#applications-and-future-work","title":"Applications and Future Work","text":"<p>Applications - Remote machinery operation (crane, excavator) - Training simulators for heavy-vehicle operators - Rehabilitation chairs for vestibular therapy - Remote telepresence in hazardous environments</p> <p>Future Enhancements 1. Integrate force sensors on actuators for closed-loop bidirectional feedback. 2. Expand motion to 6-DOF by adding heave, surge, yaw axes. 3. Replace BLE with Wi-Fi 6E or private 5G for long-distance telepresence (&gt; 500 m). 4. Incorporate AI-based adaptive control for motion prediction and compensation.  </p>"},{"location":"remote_chair/#acknowledgment","title":"Acknowledgment","text":"<p>Developed by Team 1 (Jeongsoo Pang et al.) Under SJTU UM\u2013JI Capstone Design (2025) and Builder X support, this project demonstrates that haptic telepresence can be achieved using compact mechanical systems and optimized firmware with industry-grade precision.</p>"},{"location":"bayesian/","title":"Bayesian Analysis","text":""},{"location":"bayesian/#acknowledgement","title":"Acknowledgement","text":"<ul> <li>Professor Ailin Zhang</li> <li>STAT4510: Bayesian Analysis</li> </ul>"},{"location":"bayesian/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Bayesian foundations and motivation</li> <li>Review of probability distributions and MLE</li> <li>Prior distributions and Bayesian logic</li> <li>Types and properties of priors</li> <li>Posterior inference mechanics</li> <li>Interval estimation and prediction</li> <li>Hypothesis testing and model comparison</li> <li>Loss functions and optimal decisions</li> <li>Computational methods for Bayes</li> </ol>"},{"location":"bayesian/computation/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/foundations/","title":"1. Bayesian foundations and motivation","text":"<p>Author: Jeongsoo Pang  </p>"},{"location":"bayesian/foundations/#1-concept","title":"1) Concept","text":"<p>Bayesian Statistical Inference\" Framework that uses Bayes\u2019 Rule to combine prior knowledge with observed data.</p> <p>Core Idea: Treat parameters as random variables with probability distributions rather than fixed values.</p> <p>Focus: Uncertainty quantification, knowledge updating, and probabilistic reasoning.  </p>"},{"location":"bayesian/foundations/#motivation","title":"Motivation","text":"<p>Integrates prior knowledge: Encodes existing beliefs or domain expertise.  </p> <p>Logical updating: Adjusts those beliefs as new data arrive.  </p> <p>Transparent modeling: Easy to test sensitivity to assumptions.  </p> <p>Probabilistic conclusions: Allows probability statements about parameters</p>"},{"location":"bayesian/foundations/#-ex-theres-a-95-probability-theta-lies-in-this-range","title":"- ex) \u201cThere\u2019s a 95% probability \\(\\theta\\) lies in this range\u201d","text":""},{"location":"bayesian/foundations/#3-core-equation-bayes-rule","title":"3) Core Equation (Bayes\u2019 Rule)","text":"\\[ p(\\theta \\mid x) \\;=\\; \\frac{p(x \\mid \\theta)\\,p(\\theta)}{p(x)} \\;\\propto\\; p(x \\mid \\theta)\\,p(\\theta) \\] <ul> <li>Posterior: Updated belief after observing data.  </li> <li>Likelihood: Information from observed data.  </li> <li>Prior: Belief before observing data.  </li> <li>Evidence (marginal likelihood): Normalizing constant ensuring posterior integrates to 1.  </li> </ul>"},{"location":"bayesian/foundations/#key-terms","title":"Key Terms","text":"<ul> <li>Prior \\(p(\\theta)\\): Belief about \\(\\theta\\) before seeing data.  </li> <li>Likelihood \\(p(x \\mid \\theta)\\): Probability of data given \\(\\theta\\).  </li> <li>Posterior \\(p(\\theta \\mid x)\\): Updated belief after seeing data.  </li> <li>Evidence \\(p(x)\\): Normalizing term for comparison across models.  </li> <li>Predictive Distribution \\(p(x_{\\text{new}} \\mid x)\\): Predicts unseen data, integrating over parameter uncertainty.  </li> </ul>"},{"location":"bayesian/foundations/#4-bayesian-vs-frequentist","title":"4) Bayesian vs. Frequentist","text":"Aspect Bayesian Frequentist Parameter Random variable with distribution Fixed but unknown value Data Observed once; updates belief Repeated hypothetical samples Output Posterior probability Point estimate + confidence interval Inference Probabilistic about parameters Probabilistic about data"},{"location":"bayesian/foundations/#5-where-bayesian-appears-in-mlcs","title":"5) Where \u201cBayesian\u201d Appears in ML/CS","text":"<p>Na\u00efve Bayes Classifier </p> <ul> <li>Spam detection</li> <li>uses \\(P(\\text{spam} \\mid \\text{email}) \\propto P(\\text{email} \\mid \\text{spam})\\,P(\\text{spam})\\).  </li> </ul> <p>Bayesian Networks</p> <ul> <li>Directed Acyclic Graphs (DAGs) modeling dependencies among variables</li> <li>useful for causal inference and probabilistic reasoning.  </li> </ul> <p>Bayesian Optimization </p> <ul> <li>Efficient search for global minima of costly black-box functions</li> <li>ex) hyperparameter tuning or A/B testing</li> </ul> <p>Bayesian Experimental Design</p> <ul> <li>Adaptive experiments minimizing sample or cost</li> <li>ex) early-stopping in clinical trials</li> </ul>"},{"location":"bayesian/foundations/#6-reason-for-resurgence","title":"6) Reason for Resurgence","text":"<ul> <li>Computational Power: GPUs and MCMC algorithms make complex Bayesian models practical.  </li> <li>Cultural Shift: Acceptance of subjective priors and probabilistic thinking.  </li> <li>Educational Change: Modern statistics courses incorporate Bayesian reasoning.  </li> </ul>"},{"location":"bayesian/foundations/#7-bayesian-workflow","title":"7) Bayesian Workflow","text":"<ol> <li>Specify Prior: Choose distribution for parameter \\(\\theta\\).  </li> <li>Select Likelihood: Define model generating data.  </li> <li>Compute Posterior: Combine prior and likelihood via Bayes\u2019 rule.  </li> <li>Summarize Inference: Use MAP, posterior mean, variance, or credible intervals.  </li> <li>Make Decisions: Apply posterior to decision problems (e.g., minimize expected loss).  </li> </ol>"},{"location":"bayesian/foundations/#8-mini-example-naive-bayes-for-spam","title":"8) Mini Example \u2013 Na\u00efve Bayes for Spam","text":"\\[ P(\\text{spam} \\mid \\text{email}) \\;\\propto\\; P(\\text{email} \\mid \\text{spam})\\,P(\\text{spam}) \\] <ul> <li>Assumption: Words are conditionally independent given class.  </li> <li>Result: Fast training, interpretable features, still used for baseline text classification.  </li> </ul>"},{"location":"bayesian/foundations/#12-summary-quote","title":"12) Summary Quote","text":"<p>Bayesian inference formalizes learning from data by updating prior beliefs with evidence to obtain a posterior. It enables rational, probabilistic reasoning for real-world uncertainty which is foundational to modern machine learning and decision-making.</p>"},{"location":"bayesian/hypothesis-models/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/intervals-prediction/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/loss-decisions/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/mle-review/","title":"2. Distribution and Statistical Inference","text":"<p>Author: Jeongsoo Pang  </p>"},{"location":"bayesian/mle-review/#1-overview","title":"1) Overview","text":"<p>Reviewing essential probability distributions and introduces Maximum Likelihood Estimation (MLE)</p>"},{"location":"bayesian/mle-review/#2-common-probability-distributions","title":"2) Common Probability Distributions","text":""},{"location":"bayesian/mle-review/#discrete","title":"Discrete","text":"<ul> <li> <p>Bernoulli \\((p)\\) \u2014 Binary outcome (success/failure)   $$   f(x|p) = p^x(1 - p)^{1-x}, \\quad x \\in {0,1}   $$   $$   E[X] = p, \\quad Var(X) = p(1 - p)   $$  </p> </li> <li> <p>Binomial \\((n, p)\\) \u2014 Number of successes in \\(n\\) trials   $$   P(X = x) = \\binom{n}{x} p^x (1 - p)^{n-x}   $$   $$   E[X] = np, \\quad Var(X) = np(1 - p)   $$  </p> </li> <li> <p>Negative Binomial \\((r, p)\\) \u2014 Trials until \\(r\\)-th success   $$   P(X = x) = \\binom{x-1}{r-1} p^r (1 - p)^{x-r}   $$   $$   E[X] = \\frac{r}{p}, \\quad Var(X) = \\frac{r(1 - p)}{p^2}   $$  </p> </li> <li> <p>Geometric \\((p)\\) \u2014 Trials until the first success   $$   P(X = x) = (1 - p)^{x-1}p, \\quad x = 1, 2, \\dots   $$   $$   E[X] = \\frac{1}{p}, \\quad Var(X) = \\frac{1 - p}{p^2}   $$  </p> </li> <li> <p>Poisson \\((\\lambda)\\) \u2014 Counts events in a time interval   $$   f(x|\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0,1,2,\\dots   $$   $$   E[X] = Var(X) = \\lambda   $$  </p> </li> </ul>"},{"location":"bayesian/mle-review/#continuous","title":"Continuous","text":"<ul> <li> <p>Uniform \\((\\theta_1, \\theta_2)\\)   $$   f(x) = \\frac{1}{\\theta_2 - \\theta_1}, \\quad x \\in [\\theta_1, \\theta_2]   $$   $$   E[X] = \\frac{\\theta_1 + \\theta_2}{2}, \\quad Var(X) = \\frac{(\\theta_2 - \\theta_1)^2}{12}   $$  </p> </li> <li> <p>Normal \\((\\mu, \\sigma^2)\\)   $$   f(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\Big( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\Big)   $$   $$   E[X] = \\mu, \\quad Var(X) = \\sigma^2   $$  </p> </li> <li> <p>Exponential \\((\\lambda)\\)   $$   f(x|\\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0   $$   $$   E[X] = \\frac{1}{\\lambda}, \\quad Var(X) = \\frac{1}{\\lambda^2}   $$  </p> </li> <li> <p>Gamma \\((\\alpha, \\beta)\\)   $$   f(x|\\alpha,\\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}, \\quad x \\ge 0   $$   $$   E[X] = \\frac{\\alpha}{\\beta}, \\quad Var(X) = \\frac{\\alpha}{\\beta^2}   $$</p> </li> <li> <p>Beta \\((\\alpha, \\beta)\\)   $$   f(x|\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} x^{\\alpha - 1} (1 - x)^{\\beta - 1}, \\quad 0 \\le x \\le 1   $$   $$   E[X] = \\frac{\\alpha}{\\alpha + \\beta}, \\quad Var(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}   $$  </p> </li> </ul>"},{"location":"bayesian/mle-review/#3-mle-maximum-likelihood-estimation","title":"3) MLE \u2013 Maximum Likelihood Estimation","text":"<p>Goal: Estimate parameters \\(\\theta\\) that make observed data most probable. Likelihood: $$ L(\\theta; x) = f(x|\\theta) $$ Log-likelihood: $$ \\ell(\\theta) = \\ln L(\\theta) = \\sum_i \\ln f(x_i|\\theta) $$ MLE estimate: $$ \\hat{\\theta}{MLE} = \\arg\\max\\theta \\ell(\\theta) $$  </p> <p>Example: Coin flips $$ L(p; x) = \\binom{n}{x}p^x(1 - p)^{n - x} $$ $$ \\ell(p) = x\\ln p + (n - x)\\ln(1 - p) $$ $$ \\frac{d\\ell}{dp}=0 \\Rightarrow \\hat{p}_{MLE} = \\frac{x}{n} $$  </p>"},{"location":"bayesian/mle-review/#4-transition-to-bayesian","title":"4) Transition to Bayesian","text":"<p>Frequentist treats \\(\\theta\\) as fixed. Bayesian treats \\(\\theta\\) as random with prior \\(p(\\theta)\\): $$ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\propto p(x|\\theta)p(\\theta) $$  </p> <p>This connects directly to Lecture 3: Priors and Bayesian Updating.</p>"},{"location":"bayesian/posterior-inference/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/priors-logic/","title":"3. Prior Distributions (Prior I)","text":"<p>Author: Jeongsoo Pang  </p>"},{"location":"bayesian/priors-logic/#1-recap-posterior-validity","title":"1) Recap &amp; Posterior Validity","text":"<ul> <li>Bayes (discrete): \\(P(A\\mid B)=\\dfrac{P(B\\mid A)P(A)}{P(B)}\\propto P(B\\mid A)P(A)\\) </li> <li>Bayes (continuous): \\(p(\\theta\\mid x)=\\dfrac{p(x\\mid\\theta)p(\\theta)}{p(x)}\\propto p(x\\mid\\theta)p(\\theta)\\) with \\(p(x)=\\int p(x\\mid\\theta)p(\\theta)\\,d\\theta\\) </li> <li>Proper posterior is required: \\(\\sum_\\theta p(\\theta\\mid x)=1\\) (discrete) or \\(\\int p(\\theta\\mid x)d\\theta=1\\) (continuous).  </li> </ul> <p>Useful identities - Odds form: \\(\\displaystyle \\frac{P(\\theta_1\\mid Y)}{P(\\theta_2\\mid Y)}=\\frac{P(\\theta_1)P(Y\\mid\\theta_1)}{P(\\theta_2)P(Y\\mid\\theta_2)}\\) - Total expectation: \\(E[U]=E_V\\!\\big[E[U\\mid V]\\big]\\) - Total variance: \\(\\mathrm{Var}(U)=E[\\mathrm{Var}(U\\mid V)]+\\mathrm{Var}(E[U\\mid V])\\) </p>"},{"location":"bayesian/priors-logic/#2-what-is-a-prior","title":"2) What is a Prior","text":"<ul> <li>Priors encode information before observing \\(x\\).  </li> <li>Not all priors are \u201csubjective\u201d: can be chosen by objective principles (e.g., invariance, Jeffreys), mathematical convenience, decision-theoretic arguments, or elicitation from experts.  </li> <li>Families: conjugate, informative/weakly-informative, non/informative, reference, Jeffreys, improper, invariant, nonparametric.  </li> </ul>"},{"location":"bayesian/priors-logic/#3-conjugate-priors","title":"3) Conjugate Priors","text":"<p>A prior \\(\\pi(\\theta)\\) is conjugate to likelihood \\(p(x\\mid\\theta)\\) if the posterior \\(\\pi(\\theta\\mid x)\\propto p(x\\mid\\theta)\\pi(\\theta)\\) is in the same family as \\(\\pi(\\theta)\\).  </p> <p>Benefit: closed-form posteriors, simple hyperparameter updates.</p>"},{"location":"bayesian/priors-logic/#4-core-conjugate-pairs","title":"4) Core Conjugate Pairs","text":""},{"location":"bayesian/priors-logic/#normal-mean","title":"Normal mean","text":"<ul> <li>Model: \\(X\\sim\\mathcal N(\\mu,\\sigma^2)\\), \\(\\sigma^2\\) known  </li> <li>Prior: \\(\\mu\\sim\\mathcal N(\\nu,\\eta^2)\\) </li> <li>Posterior:   $$   \\mu\\mid X \\sim \\mathcal N!\\left(   \\frac{\\eta^2 X+\\sigma^2 \\nu}{\\eta^2+\\sigma^2}\\;,\\;   \\frac{\\eta^2\\sigma^2}{\\eta^2+\\sigma^2}   \\right)   $$   Interpretation: posterior mean is a precision-weighted average of \\(X\\) and \\(\\nu\\):   $$   \\mu\\mid X \\sim \\mathcal N!\\left(   \\frac{X/\\sigma^2+\\nu/\\eta^2}{1/\\sigma^2+1/\\eta^2}\\;,\\;   \\frac{1}{1/\\sigma^2+1/\\eta^2}   \\right)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#bernoullibinomial","title":"Bernoulli/Binomial","text":"<ul> <li>Bernoulli \\(X_i\\sim\\mathrm{Ber}(\\theta)\\) or \\(Y\\sim\\mathrm{Bin}(n,\\theta)\\) </li> <li>Prior: \\(\\theta\\sim\\mathrm{Beta}(\\alpha,\\beta)\\) </li> <li>Let \\(S=\\sum_i X_i\\) (or \\(y\\)). Posterior:   $$   \\theta\\mid X \\sim \\mathrm{Beta}!\\big(\\alpha+S,\\;\\beta+n-S\\big)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#geometric","title":"Geometric","text":"<ul> <li>\\(X\\sim\\mathrm{Geom}(\\theta)\\) (trials until first success, support \\(1,2,\\dots\\))  </li> <li>Prior: \\(\\theta\\sim\\mathrm{Beta}(\\alpha,\\beta)\\) </li> <li>Posterior:   $$   \\theta\\mid X=x \\sim \\mathrm{Beta}!\\big(\\alpha+1,\\;\\beta+x-1\\big)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#poisson","title":"Poisson","text":"<ul> <li>\\(X\\sim\\mathrm{Poisson}(\\lambda)\\) </li> <li>Prior: \\(\\lambda\\sim\\mathrm{Gamma}(\\alpha,\\beta)\\) (shape\u2013rate)  </li> <li>Posterior:   $$   \\lambda\\mid X=x \\sim \\mathrm{Gamma}!\\big(\\alpha+x,\\;\\beta+1\\big)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#5-variance-priors-joint-conjugacy-for-normal","title":"5) Variance Priors &amp; Joint Conjugacy for Normal","text":""},{"location":"bayesian/priors-logic/#inverse-gamma-for-variance-mean-known","title":"Inverse-Gamma for variance (mean known)","text":"<ul> <li>\\(X\\sim\\mathcal N(\\mu,\\sigma^2)\\) with \\(\\mu\\) known  </li> <li>Prior: \\(\\sigma^2\\sim\\mathrm{InvGamma}(\\alpha,\\beta)\\) </li> <li>Posterior:   $$   \\sigma^2 \\mid X \\sim \\mathrm{InvGamma}!\\left(\\alpha+\\tfrac12,\\;\\beta+\\tfrac12(X-\\mu)^2\\right)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#normalinverse-gamma-mean-variance-unknown","title":"Normal\u2013Inverse-Gamma (mean &amp; variance unknown)","text":"<ul> <li>Likelihood: \\(X\\mid\\mu,\\sigma^2\\sim\\mathcal N(\\mu,\\sigma^2)\\) </li> <li>Prior:    $$   \\mu\\mid\\sigma^2 \\sim \\mathcal N!\\Big(\\mu_0,\\;\\sigma^2/\\kappa_0\\Big),    \\qquad \\sigma^2 \\sim \\mathrm{InvGamma}(\\alpha_0,\\beta_0)   $$</li> <li>Posterior updates:   $$   \\kappa_n=\\kappa_0+1,\\quad    \\mu_n=\\frac{\\kappa_0\\mu_0+X}{\\kappa_0+1},\\quad    \\alpha_n=\\alpha_0+\\tfrac12,\\quad   \\beta_n=\\beta_0+\\tfrac12\\,\\frac{\\kappa_0}{\\kappa_0+1}(X-\\mu_0)^2   $$   and   $$   \\mu\\mid\\sigma^2,X\\sim\\mathcal N(\\mu_n,\\sigma^2/\\kappa_n),\\quad    \\sigma^2\\mid X\\sim\\mathrm{InvGamma}(\\alpha_n,\\beta_n)   $$</li> </ul>"},{"location":"bayesian/priors-logic/#6-multinomialdirichlet","title":"6) Multinomial\u2013Dirichlet","text":"<ul> <li>Likelihood: \\(X=(X_1,\\dots,X_k)\\sim\\mathrm{Multinomial}(n;\\,p_1,\\dots,p_k)\\) </li> <li>Prior: \\(\\mathbf p\\sim\\mathrm{Dirichlet}(\\alpha_1,\\dots,\\alpha_k)\\) </li> <li>Posterior: \\(\\mathbf p\\mid X\\sim\\mathrm{Dirichlet}(\\alpha_1+X_1,\\dots,\\alpha_k+X_k)\\) </li> </ul>"},{"location":"bayesian/priors-logic/#7-exponential-family-view-why-conjugacy-is-common","title":"7) Exponential Family View (why conjugacy is common)","text":"<p>Many likelihoods satisfy $$ p(x\\mid\\eta)=h(x)\\exp{\\eta^\\top t(x)-g(\\eta)} $$ A generic conjugate prior is $$ \\pi(\\eta\\mid\\tau,\\rho)\\propto \\exp{\\tau^\\top\\eta-\\rho\\,g(\\eta)} $$ which yields posterior $$ \\pi(\\eta\\mid x,\\tau,\\rho)\\propto \\exp{(\\tau+t(x))^\\top\\eta-(\\rho+1)g(\\eta)} $$</p> <p>Example (Poisson): with \\(\\eta=\\log\\lambda\\), the generic form reduces to Gamma prior on \\(\\lambda\\).</p>"},{"location":"bayesian/priors-logic/#8-choosing-hyperparameters","title":"8) Choosing Hyperparameters","text":"<p>1) Match moments: choose prior \\((\\alpha,\\beta,\\ldots)\\) to hit desired prior mean/variance. 2) Quantiles: set hyperparameters so prior places specific mass at meaningful cutoffs. 3) Hierarchical Bayes: put priors on hyperparameters to learn them from data. 4) Empirical Bayes: estimate hyperparameters from data (plug-in).  </p>"},{"location":"bayesian/priors-logic/#one-sentence-note","title":"One-sentence Note","text":"<p>Conjugate priors turn Bayes\u2019 rule into simple algebra: prior hyperparameters update with sufficient statistics, giving closed-form posteriors and crisp interpretations (precision-weighted averaging, count-additivity).</p>"},{"location":"bayesian/priors-types/","title":"4. Prior Distributions II (Advanced Priors)","text":"<p>Author: Jeongsoo Pang  </p>"},{"location":"bayesian/priors-types/#1-overview","title":"1) Overview","text":"<p>Extending prior concepts into improper, noninformative, Jeffreys, weakly informative, and reference priors, exploring how parameterization affects prior meaning and inference</p>"},{"location":"bayesian/priors-types/#2-categories-of-priors","title":"2) Categories of Priors","text":"Type of Prior Definition / Concept Key Properties &amp; Purpose Typical Example / Use Case Conjugate Prior Prior family that yields a posterior in the same distributional family as the prior. Provides closed-form posterior; simple parameter updates; algebraically convenient. Beta\u2013Binomial, Gamma\u2013Poisson, Normal\u2013Normal. Improper Prior A prior that does not integrate to 1, but can still produce a proper posterior. Useful when minimal information is available; must ensure posterior integrates to 1. Flat prior \\(\\pi(\\mu)\\propto1\\) for Normal mean. Informative Prior Reflects strong domain knowledge or expert belief. Low variance, sharp peak; dominates likelihood when data are scarce. Prior on disease rate based on previous clinical data. Weakly Informative Prior A vague but proper prior that regularizes inference without overwhelming the data. Improves stability; balances between noninformative and informative. \\(\\text{Beta}(1,1)\\) or \\(\\text{Beta}(0.5,0.5)\\) for Binomial \\(\\theta\\). Noninformative Prior Attempts to make inference rely almost entirely on the data likelihood. No universally correct form; depends on parameterization; often used in objective Bayes. Uniform\\((0,1)\\) for \\(\\theta\\) in Binomial\\((n,\\theta)\\). Reference Prior Maximizes mutual information between parameter \\(\\theta\\) and data. Objective choice; designed to maximize expected information gain. Used for multidimensional parameters in objective Bayes. Jeffreys Prior Defined as \\(\\pi(\\theta)\\propto\\sqrt{I(\\theta)}\\), where \\(I(\\theta)\\) is Fisher information. Invariant under reparameterization; noninformative in an information-theoretic sense. For Bernoulli \\(p\\), \\(\\pi(p)\\propto[p(1-p)]^{-1/2}\\) (Beta\\((1/2,1/2)\\)). Invariant / Nonparametric Prior Defined over transformations or infinite-dimensional parameter spaces. Used in flexible models (e.g., Gaussian Process, Dirichlet Process). Nonparametric Bayes, hierarchical GP models."},{"location":"bayesian/priors-types/#3-proper-vs-improper-priors","title":"3) Proper vs. Improper Priors","text":"<p>A prior \\(p(\\theta)\\) is proper if it integrates to 1: $$ \\int p(\\theta)\\,d\\theta = 1 $$ Otherwise, it is improper: $$ \\int p(\\theta)\\,d\\theta = \\infty $$</p> <ul> <li>An improper prior is acceptable only if the resulting posterior is proper.  </li> <li>If the posterior is also improper, the prior must be discarded.  </li> </ul> <p>Example: For Binomial \\(y\\sim Bin(n,\\theta)\\) and prior \\(\\text{Beta}(0,0)\\), the posterior \\(\\text{Beta}(y, n-y)\\) is proper when \\(y,n-y&gt;0\\).  </p> <p>Another example: For \\(X_i\\sim N(\\mu,\\sigma^2)\\) with \\(\\pi(\\mu)\\propto1\\), $$ \\mu|x\\sim N(\\bar{x},\\sigma^2/n) $$ which is proper\u2014so flat prior \\(\\pi(\\mu)\\propto1\\) works fine</p>"},{"location":"bayesian/priors-types/#4-informative-vs-noninformative-priors","title":"4) Informative vs. Noninformative Priors","text":"<ul> <li>Informative prior: Low variance, expresses strong prior knowledge.  </li> <li>Vague/diffuse prior: High variance, weak information.  </li> <li>Flat prior: Constant over parameter space (e.g., \\(\\pi(\\theta)\\propto1\\)), fully noninformative.</li> </ul> <p>Noninformative priors: - Aim to let data \u201cspeak for itself.\u201d - Used in objective Bayes when little prior knowledge exists. - No universally agreed definition \u2014 depends on model parameterization.  </p>"},{"location":"bayesian/priors-types/#5-parameterization-sensitivity","title":"5) Parameterization Sensitivity","text":"<p>Noninformative priors depend heavily on how parameters are defined.</p>"},{"location":"bayesian/priors-types/#example-1-binomial-model","title":"Example 1 \u2014 Binomial model","text":"<p>For \\(y\\sim Bin(n,\\theta)\\):</p> <ul> <li>Flat prior: \\(\\theta\\sim Uniform(0,1)\\) or \\(\\text{Beta}(1,1)\\)   $$   \\Rightarrow \\theta|y\\sim Beta(y+1, n-y+1)   $$</li> </ul>"},{"location":"bayesian/priors-types/#example-2-odds-transformation","title":"Example 2 \u2014 Odds transformation","text":"<p>Let \\(o=\\frac{\\theta}{1-\\theta}\\) and assume flat prior on \\(o\\) (\\(p(o)\\propto1\\)). Then the induced prior on \\(\\theta\\) is: $$ p(\\theta)\\propto \\frac{1}{(1-\\theta)^2}, \\quad 0&lt;\\theta&lt;1 $$ This favors \\(\\theta\\) near 1 and is improper since \\(\\int_0^1 (1-\\theta)^{-2}d\\theta=\\infty\\).</p> <p>Posterior under Binomial likelihood: $$ p(\\theta|y)\\propto \\theta^y(1-\\theta)^{n-y-2} \\Rightarrow \\theta|y\\sim Beta(y+1, n-y-1) $$ Posterior becomes improper when \\(n-y\\le1\\):contentReference[oaicite:3]{index=3}.</p>"},{"location":"bayesian/priors-types/#example-3-log-odds-reparameterization","title":"Example 3 \u2014 Log-odds reparameterization","text":"<p>Let \\(\\rho=\\log\\!\\left(\\frac{\\theta}{1-\\theta}\\right)\\) and assume \\(\\rho\\sim Uniform(-\\infty,\\infty)\\). Then, $$ p(\\theta)\\propto \\frac{1}{\\theta(1-\\theta)} $$ Posterior becomes: $$ \\theta|y\\sim Beta(y, n-y) $$ Insight: \u201cFlat\u201d priors depend on the scale; uniform in \\(\\theta\\), \\(o\\), or \\(\\rho\\) lead to very different beliefs</p>"},{"location":"bayesian/priors-types/#6-jeffreys-prior","title":"6) Jeffreys\u2019 Prior","text":"<p>Motivation: A prior should remain \u201cuninformative\u201d under smooth reparameterization.  </p> <p>Jeffreys\u2019 Rule: $$ \\pi(\\theta)\\propto \\sqrt{I(\\theta)} $$ where \\(I(\\theta)\\) is the Fisher information: $$ I(\\theta)=E_\\theta!\\left[\\left(\\frac{\\partial}{\\partial\\theta}\\log L(\\theta;X)\\right)^2\\right] $$</p> <ul> <li>Reflects information content in the data about \\(\\theta\\).  </li> <li>Gives higher weight where the likelihood is most sensitive to \\(\\theta\\).  </li> <li>Completely determined by the likelihood (no subjective input).  </li> </ul>"},{"location":"bayesian/priors-types/#example-bernoulli","title":"Example: Bernoulli","text":"<p>For \\(X_i\\sim Bern(p)\\), $$ I(p)=\\frac{n}{p(1-p)} \\Rightarrow \\pi(p)\\propto [p(1-p)]^{-1/2} $$ Hence, Jeffreys\u2019 prior: $$ p(p)\\sim Beta!\\left(\\frac{1}{2},\\frac{1}{2}\\right) $$</p>"},{"location":"bayesian/priors-types/#example-normal-mean-2-known","title":"Example: Normal mean (\u03c3\u00b2 known)","text":"<p>For \\(X\\sim N(\\mu,\\sigma^2)\\), $$ I(\\mu)=\\frac{1}{\\sigma^2} \\Rightarrow \\pi(\\mu)\\propto1 $$ \u21d2 Jeffreys\u2019 prior is uniform (improper)</p>"},{"location":"bayesian/priors-types/#7-weakly-informative-priors","title":"7) Weakly Informative Priors","text":"<ul> <li>Use proper but vague priors to regularize estimates and avoid improper posteriors.  </li> <li>Example: \\(\\text{Beta}(1,1)\\) or \\(\\text{Beta}(0.5,0.5)\\) for Binomial \u03b8.  </li> <li>Behaves similarly to \\(\\text{Beta}(0,0)\\) (improper) but ensures posterior propriety.  </li> <li>Common in modern Bayesian modeling (e.g., logistic regression, hierarchical models)</li> </ul>"},{"location":"bayesian/priors-types/#8-reference-priors","title":"8) Reference Priors","text":"<p>A reference prior maximizes mutual information between data and parameters.</p> \\[ p^*(\\theta) = \\arg\\max_{p(\\theta)} I(\\Theta, T) \\] <p>where</p> \\[ I(\\Theta, T) = \\iint p(\\theta, t)\\, \\log\\!\\frac{p(\\theta, t)}{p(\\theta)p(t)}\\,d\\theta\\,dt \\] <ul> <li>Essentially: choose the prior that maximizes the expected information gain from data.  </li> <li>Used in objective Bayesian analysis, especially for multi-parameter models.</li> </ul>"},{"location":"bayesian/priors-types/#summary-quote","title":"Summary Quote","text":"<p>A prior\u2019s \u201cuninformativeness\u201d depends on the parameter scale. Jeffreys\u2019 rule and weakly informative priors balance mathematical invariance and practical stability\u2014ensuring the data, not arbitrary parameterization, drives inference.</p>"},{"location":"images/","title":"IMAGES","text":""},{"location":"os/","title":"Operating Systems","text":""},{"location":"os/#acknowledgement","title":"Acknowledgement","text":"<ul> <li>Professor Manuel Charlemagne</li> <li>ECE4820 Introduction to Operation Systems</li> </ul>"},{"location":"os/#table-of-contents","title":"Table of Contents","text":""},{"location":"os/#1-critical-section","title":"1. Critical Section","text":""},{"location":"os/critical_section/","title":"OS : Critical Section","text":"<ul> <li>Author: Jeongsoo Pang  </li> </ul>"},{"location":"os/critical_section/#1-concept","title":"1) Concept","text":"<ul> <li>Critical Section (CS): A section of code that accesses shared state, however, must not be executed by more than one thread/process at the same time.</li> <li>Goal: Prevent race conditions (nondeterministic bugs caused by interleavings of reads/writes to shared data).. Classic symptoms of a race:</li> <li>Lost updates (A writes, then B overwrites).</li> <li>Read of inconsistent/partial state.</li> <li>Occasional test flakiness that \u201cgoes away\u201d when adding prints or sleeps.</li> </ul>"},{"location":"os/critical_section/#2-the-critical-section-problem","title":"2) The \u201cCritical Section Problem\u201d","text":"<p>A correct solution enforces 3-properties (by Dijkstra):</p> <p>1. Mutual Exclusion  - At most one thread is inside the CS at any time.</p> <p>2. Progress  - If no thread is inside the CS, one of the threads wishing to enter must be able to proceed.</p> <p>3. Bounded Waiting (No Starvation)  - There is a finite bound on the number of times other threads can enter their CS after a thread has requested entry and before it gets in.</p>"},{"location":"os/critical_section/#3-the-4-part-structure-of-concurrent-code","title":"3) The 4-part Structure of Concurrent Code","text":"<pre><code>1. Entry Section -&gt; 2. Critical Section -&gt; 3. Exit Section -&gt; 4. Remainder Section\n (check/acquire)      (touch shared)          (release)          (private work)\n</code></pre> <ul> <li>Entry: Acquire the right to enter the CS (lock, protocol).</li> <li>Critical Section: Access/modify shared state.</li> <li>Exit: Release the right (unlock, clear flags).</li> <li>Remainder: Do private or non-shared work.</li> </ul>"},{"location":"os/critical_section/#5-classic-software-only-algorithms","title":"5) Classic Software-Only Algorithms","text":"<p>Work on sequential consistency and shared memory. Useful to understand progress/bounded waiting.</p>"},{"location":"os/critical_section/#51-petersons-algorithm-2-threads","title":"5.1 Peterson\u2019s Algorithm (2 threads)","text":"<pre><code>// Shared\nvolatile bool want[2] = {false, false};\nvolatile int turn = 0;\n\n// Thread i (i in {0,1}):\nwant[i] = true;\nturn = 1 - i;\nwhile (want[1 - i] &amp;&amp; turn == 1 - i) {\n/* busy wait /\n}\n\n/ ---- Critical Section ---- */\nwant[i] = false;\n</code></pre> <ul> <li>Satisfies mutual exclusion, progress, bounded waiting (under SC).</li> <li>Mostly pedagogical; compilers/CPUs reorder \u2192 needs memory barriers in practice.</li> </ul>"},{"location":"os/critical_section/#6-hardware-support-atomic-primitives","title":"6) Hardware Support: Atomic Primitives","text":"<p>Modern solutions rely on atomic read-modify-write (RMW) instructions: - Test-and-Set (TAS):</p> <pre><code>bool test_and_set(bool *x) {\nbool old = *x;\n*x = true;\nreturn old;\n}\n</code></pre> <ul> <li>Compare-and-Swap (CAS): <code>CAS(addr, expected, new)</code> atomically does:</li> </ul> <pre><code>if (*addr == expected) { *addr = new; return true; } else return false;\n</code></pre> <ul> <li>Fetch-and-Add (FAA), XCHG, or LL/SC (Load-Linked/Store-Conditional).</li> </ul> <p>Memory Ordering: Many CPUs are not sequentially consistent. Use: - Acquire on loads that observe a lock; Release on stores that unlock. - Fences/barriers as needed (e.g., <code>atomic_thread_fence(memory_order_seq_cst)</code>). - In C/C++ atomics, pair <code>memory_order_acquire</code> with <code>memory_order_release</code>.</p>"},{"location":"os/critical_section/#7-locks-and-locking-primitives","title":"7) Locks and Locking Primitives","text":""},{"location":"os/critical_section/#71-spinlock-busy-wait","title":"7.1 Spinlock (busy-wait)","text":"<p>Use when CS is very short and threads are truly running on different CPUs.</p> <pre><code>// TAS spinlock\nstd::atomic&lt;bool&gt; locked{false};\n\nvoid lock() {\nwhile (locked.exchange(true, std::memory_order_acquire)) {\n// spin\n}\n}\n\nvoid unlock() {\nlocked.store(false, std::memory_order_release);\n}\n</code></pre> <ul> <li>Pros: Simple, low latency for tiny CS.</li> <li>Cons: Wastes CPU cycles; terrible if CS may block/sleep or be long. Improvements:</li> </ul>"},{"location":"os/critical_section/#72-mutex-sleeping-lock","title":"7.2 Mutex (sleeping lock)","text":"<p>Use when CS can be longer or a thread may block inside CS.</p> <ul> <li>If lock unavailable, the kernel places thread on a wait queue.</li> <li>Often features: fairness, priority inheritance, timed trylock.</li> </ul> <pre><code>pthread_mutex_lock(&amp;m);\n\n/* critical section */\npthread_mutex_unlock(&amp;m);\n</code></pre>"},{"location":"os/critical_section/#73-readerwriter-sharedexclusive-locks","title":"7.3 Reader\u2013Writer (Shared/Exclusive) Locks","text":"<ul> <li>Multiple readers can enter concurrently (writers need exclusivity).</li> <li>Variants: Reader-preferred, Writer-preferred, Fair.</li> <li>Be careful: Reader preference can starve writers.</li> </ul>"},{"location":"os/critical_section/#8-higher-level-constructs","title":"8) Higher-Level Constructs","text":""},{"location":"os/critical_section/#81-semaphores","title":"8.1 Semaphores","text":"<ul> <li>Counting semaphore: integer \u2265 0 with <code>P()/wait()</code> and <code>V()/signal()</code>.</li> <li>Binary semaphore \u2248 mutex (but with different semantics = no ownership).</li> <li>Great for resource counting and producer\u2013consumer.</li> </ul> <pre><code>semaphore empty = N; // free slots\nsemaphore full = 0; // filled slots\nmutex m = 1;\n\nproducer:\nwait(empty);\nwait(m);\nput(item);\nsignal(m);\nsignal(full);\n\nconsumer:\nwait(full);\nwait(m);\nget(item);\nsignal(m);\nsignal(empty);\n</code></pre>"},{"location":"os/critical_section/#82-monitors","title":"8.2 Monitors","text":"<ul> <li>Language-level construct: only one thread executes a monitor\u2019s method at a time (implicit mutual exclusion).</li> <li>Condition variables (CVs) inside a monitor provide waiting and signaling:   -\\ <code>wait(cv)</code>: atomically releases the monitor lock and blocks.   -\\ <code>signal(cv)</code> or <code>broadcast(cv)</code>: wake one/all waiting threads.</li> </ul>"},{"location":"os/critical_section/#9-kernel-vs-user-space","title":"9) Kernel vs. User Space","text":"Aspect User Space Kernel Space Notes Typical primitives <code>pthread_mutex</code>, <code>pthread_rwlock</code>, <code>pthread_cond</code>, semaphores (POSIX) spinlocks, mutexes (<code>mutex</code>/<code>rwsem</code>), RCU, seqlocks, futex backends User locks often use futex to sleep in kernel on contention. Contention behavior Starts in user mode; enters kernel only when contended (futex wait/wake) Fully managed by kernel; may spin or sleep based on lock type and context Minimizes syscalls on fast path in user space. Preemption/interrupts Cannot disable either Can disable preemption/IRQs for very short CS on a CPU Disabling IRQs \u2260 cross-CPU exclusion. Critical-section duration Usually longer; may block (I/O, syscalls) Very short for spin; sleeping locks for longer/IO-touching regions Spin only for microseconds. Sleep inside CS? Allowed with mutex/RW locks (not with user spinlocks) Never while holding spinlocks; allowed with sleeping locks Holding a spinlock + sleep \u2192 bug. Fairness/starvation controls Fair mutexes; RW lock policies (reader/writer pref, fair) Ticket/MCS locks, prio-aware policies Kernel often provides stronger fairness knobs. Priority inversion handling Via priority inheritance (PI) mutexes (e.g., <code>pthread_mutexattr_setprotocol</code>) PI/PCP mechanisms on kernel mutexes Use PI for real-time threads. Memory ordering C/C++ atomics with acquire/release + fences Architecture-specific barriers; lock/unlock imply ordering Same principles; different primitives. Wakeups Futex wake by kernel (targeted wake) Wait-queues, <code>wake_up*</code> APIs Both try to avoid thundering herd. Examples App queues, caches, work pools Scheduler runqueues, inode caches, networking fast paths Choose primitives by context."},{"location":"os/critical_section/#10-worked-examples","title":"10) Worked Examples","text":""},{"location":"os/critical_section/#101-protecting-a-shared-counter","title":"10.1 Protecting a Shared Counter","text":"<p>Incorrect:</p> <pre><code>// race: i++ is read-modify-write\ni++;\n</code></pre> <p>Correct (mutex):</p> <pre><code>pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid increment() {\n  pthread_mutex_lock(&amp;m);\n  i++;\n  pthread_mutex_unlock(&amp;m);\n}\n</code></pre> <p>Correct (atomic, lock-free):</p> <pre><code>std::atomic&lt;int&gt; i{0};\nvoid increment() {\n  i.fetch_add(1, std::memory_order_relaxed);\n}\n</code></pre> <p>If other invariants exist around <code>i</code>, you might need stronger ordering.</p>"},{"location":"os/critical_section/#102-producerconsumer-with-condition-variables-mesa-semantics","title":"10.2 Producer\u2013Consumer with Condition Variables (Mesa semantics)","text":"<pre><code>std::queue&lt;int&gt; q;\nconst size_t CAP = 1024;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t not_full = PTHREAD_COND_INITIALIZER;\npthread_cond_t not_empty = PTHREAD_COND_INITIALIZER;\n\nvoid produce(int item) {\n  pthread_mutex_lock(&amp;m);\n  while (q.size() == CAP)\n  pthread_cond_wait(&amp;not_full, &amp;m);\n  q.push(item);\n  pthread_cond_signal(&amp;not_empty);\n  pthread_mutex_unlock(&amp;m);\n}\n\nint consume() {\n  pthread_mutex_lock(&amp;m);\n  while (q.empty())\n  pthread_cond_wait(&amp;not_empty, &amp;m);\n  int item = q.front(); q.pop();\n  pthread_cond_signal(&amp;not_full);\n  pthread_mutex_unlock(&amp;m);\n  return item;\n}\n</code></pre>"},{"location":"os/critical_section/#11-notes","title":"11) NOTES","text":"<ul> <li>Keep CS minimal : do not call unbounded or blocking operations inside.</li> <li>Clearly document lock ownership and lock ordering in comments.</li> <li>Use RAII/scoped guards to avoid forgotten unlocks.</li> <li>Prefer condition variables over ad-hoc sleeps : always <code>while (!cond) wait</code>.</li> <li>Consider priority inversion in real-time systems: enable priority inheritance.</li> <li>On weakly ordered CPUs, ensure correct acquire/release semantics.</li> </ul>"},{"location":"os/critical_section/#12-cheat-sheet-table-alternative","title":"12) Cheat-Sheet (table alternative)","text":"Situation Pick Short CS on multicore? Spin (TAS + backoff or MCS) May block / long CS? Mutex or RW lock Multiple readers, rare writers? Reader\u2013Writer lock or RCU Pool of N resources? Counting semaphore Waking sleepers on condition? CV with <code>while (!cond) wait</code> Avoid deadlock Global lock order + trylock fallback"},{"location":"os/lec2_process_thread/","title":"Processes and Threads","text":""},{"location":"os/lec2_process_thread/#1-process-concept","title":"1) Process Concept","text":"<p>A Process is an instance of a program in execution (basic unit of work in a system)</p> <p>Each process includes:</p> <ul> <li>Program Code (Text Section)</li> <li>Program Counter (PC) \u2013 current instruction address</li> <li>Stack \u2013 function calls, local variables</li> <li>Data Section \u2013 global/static variables</li> <li>Heap \u2013 dynamically allocated memory</li> </ul> <p>A process is not just code, but a running entity with state, memory, and resources.</p>"},{"location":"os/lec2_process_thread/#2-process-states","title":"2) Process States","text":"State Description New The process is being created. Ready The process is waiting to be assigned to a CPU. Running Instructions are being executed. Waiting (Blocked) The process is waiting for some event (e.g., I/O completion). Terminated The process has finished execution."},{"location":"os/lec2_process_thread/#state-transitions","title":"State Transitions","text":"<p>New \u2192 Ready \u2192 Running \u2192 Waiting \u2192 Ready \u2192 Terminated</p>"},{"location":"os/lec2_process_thread/#3-process-control-block-pcb","title":"3) Process Control Block (PCB)","text":"<p>The Process Control Block (PCB) stores all information about a process.</p> Field Description Process State Running, Waiting, etc. Program Counter Next instruction address CPU Registers Saved values during context switch CPU Scheduling Info Priority, scheduling parameters Memory Management Info Page tables, segment tables Accounting Info CPU usage, process ID I/O Status Info List of open files, I/O devices <p>The PCB allows the OS to suspend and resume processes efficiently.</p>"},{"location":"os/lec2_process_thread/#4-context-switch","title":"4) Context Switch","text":"<p>A Context Switch occurs when the CPU switches from one process to another. </p> <p>The OS must:</p> <ol> <li>Save the current process\u2019s state (PCB)</li> <li>Load the next process\u2019s PCB and restore its state</li> </ol>"},{"location":"os/lec2_process_thread/#cost","title":"Cost","text":"<p>Context switching is overhead (no useful work is done during the switch)</p>"},{"location":"os/lec2_process_thread/#5-process-scheduling","title":"5) Process Scheduling","text":"<p>Scheduling determines which process runs next on the CPU.</p>"},{"location":"os/lec2_process_thread/#schedulers","title":"Schedulers","text":"Scheduler Description Long-Term Scheduler Selects which processes are admitted into memory (controls degree of multiprogramming). Short-Term Scheduler Chooses which ready process will run next (CPU scheduling). Medium-Term Scheduler Handles swapping (suspending/resuming processes)."},{"location":"os/lec2_process_thread/#ready-queue-device-queues","title":"Ready Queue &amp; Device Queues","text":"<ul> <li>Ready Queue: All processes waiting for CPU.  </li> <li>Device Queues: Processes waiting for specific I/O devices.  </li> </ul>"},{"location":"os/lec2_process_thread/#6-process-creation-and-termination","title":"6) Process Creation and Termination","text":""},{"location":"os/lec2_process_thread/#creation","title":"Creation","text":"<ul> <li>Parent process creates child via <code>fork()</code>.</li> <li>Child gets a duplicate of the parent\u2019s memory and resources.</li> </ul> <pre><code>pid_t pid = fork();\nif (pid == 0) {\n    // Child process\n    execlp(\"/bin/ls\", \"ls\", NULL);\n} else {\n    // Parent process\n    wait(NULL);\n}\n</code></pre>"},{"location":"os/lec2_process_thread/#termination","title":"Termination","text":"<ul> <li>Normal exit (exit())</li> <li>Error exit</li> <li>Killed by another process (abort())</li> <li>Cascading termination: when a parent terminates, all children are also killed.</li> </ul>"},{"location":"os/lec2_process_thread/#7-process-hierarchies","title":"7) Process Hierarchies","text":"<p>Processes form parent\u2013child relationships.</p> <p>Parent may share resources or restrict child access.</p> <p>In UNIX:</p> <ul> <li><code>ps</code> command shows parent PID (PPID).</li> <li><code>init</code> (PID 1) is ancestor of all processes.</li> </ul>"},{"location":"os/lec2_process_thread/#8-interprocess-communication-ipc","title":"8) Interprocess Communication (IPC)","text":"<p>Reasons for IPC - Data sharing - Computation speedup - Modularity (separation of concerns)</p> Model Description Shared Memory Processes share a region of memory and communicate via read/write operations. Message Passing Processes communicate via send/receive messages (used in distributed systems)."},{"location":"os/lec2_process_thread/#9-threads-the-lightweight-process","title":"9) Threads \u2013 The Lightweight Process","text":"<p>A Thread is the smallest unit of CPU utilization, consisting of:</p> <ul> <li>Thread ID</li> <li>Program Counter</li> <li>Register Set</li> <li>Stack</li> </ul> <p>Threads share within the same process:</p> <ul> <li>Code Section</li> <li>Data Section (globals, heap)</li> <li>OS Resources (files, signals, address space)</li> </ul> <p>Threads in the same process communicate cheaply via shared memory, but must synchronize to avoid races.</p>"},{"location":"os/lec2_process_thread/#10-benefits-of-multithreading","title":"10) Benefits of Multithreading","text":"Advantage Description Responsiveness UI stays responsive while background tasks (I/O, compute) proceed. Resource Sharing Threads share the same address space and resources of the process. Economy Creating/switching threads is cheaper than processes. Scalability Maps naturally onto multicore CPUs for speedups."},{"location":"os/lec2_process_thread/#11-multithreading-models","title":"11) Multithreading Models","text":"Model Mapping Pros Cons Many-to-One Many user -&gt; 1 kernel Simple, low overhead One blocks all; no true parallelism One-to-One 1 user -&gt; 1 kernel True parallelism, better isolation Higher overhead (many kernel threads) Many-to-Many Many user -&gt; Many kernel Combines benefits; flexible scheduling More complex implementation"},{"location":"os/lec2_process_thread/#12-example-pthreads","title":"12) Example: Pthreads","text":"<pre><code>#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt;\n\nvoid* print_message(void* arg) {\n    printf(\"Hello from thread!\\n\");\n    return NULL;\n}\n\nint main() {\n    pthread_t thread;\n    pthread_create(&amp;thread, NULL, print_message, NULL);\n    pthread_join(thread, NULL);\n    return 0;\n}\n</code></pre> <p>Functions:</p> <ul> <li><code>pthread_create()</code> \u2013 Create a thread</li> <li><code>pthread_join()</code> \u2013 Wait for thread to finish</li> <li><code>pthread_exit()</code> \u2013 Terminate thread</li> <li><code>pthread_yield()</code> \u2013 Yield CPU voluntarily</li> </ul> <p>A process is an independent execution unit; a thread is a lightweight component sharing the same resources. Always distinguish between process-level isolation and thread-level concurrency.</p>"},{"location":"os/lec3_ipc_synchronization/","title":"Interprocess Communication (IPC) &amp; Synchronization","text":""},{"location":"os/lec3_ipc_synchronization/#1-ipc-synchronization","title":"1) IPC &amp; Synchronization","text":"<ul> <li>Goal: Coordinate concurrent processes/threads that access shared state to avoid race conditions and ensure correctness.</li> <li>Core correctness properties (Dijkstra):</li> <li>Mutual Exclusion \u2014 At most one thread is in the Critical Section (CS).</li> <li>Progress \u2014 If no one is in CS, someone wanting to enter can eventually proceed.</li> <li>Bounded Waiting (No Starvation) \u2014 A thread will not wait forever to enter CS.</li> </ul> <p>Race Condition: Outcome depends on interleaving of operations on shared data.</p>"},{"location":"os/lec3_ipc_synchronization/#2-critical-section-patterns","title":"2) Critical Section Patterns","text":""},{"location":"os/lec3_ipc_synchronization/#21-structure","title":"2.1 Structure","text":"<pre><code>entry_section();   // acquire\n/* Critical Section: read/write shared data */\nexit_section();    // release\nremainder_section();\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#22-approaches","title":"2.2 Approaches","text":"<ul> <li> <p>Software-only: Strict alternation, Peterson\u2019s Algorithm (2 threads), bakery algorithm (N threads).</p> </li> <li> <p>Hardware support: Atomic Instructions (Test-and-Set, Compare-and-Swap, Fetch-and-Add), Disable Interrupts (kernel-only, short).</p> </li> <li> <p>OS primitives: Mutex, Semaphore, Condition Variable, Monitor.</p> </li> </ul>"},{"location":"os/lec3_ipc_synchronization/#3-mutex-condition-variables","title":"3. Mutex &amp; Condition Variables","text":""},{"location":"os/lec3_ipc_synchronization/#31-mutex","title":"3.1 Mutex","text":"<p>Ensures mutual exclusion around a CS.</p> <pre><code>pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\npthread_mutex_lock(&amp;m);\n/* Critical Section */\npthread_mutex_unlock(&amp;m);\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#32-condition-variable-cv","title":"3.2 Condition Variable (CV)","text":"<p>Wait for a predicate to become true, atomically releasing &amp; reacquiring the mutex.</p> <pre><code>pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t  cv = PTHREAD_COND_INITIALIZER;\nint ready = 0;\n\nvoid wait_until_ready() {\n    pthread_mutex_lock(&amp;m);\n    while (!ready) // ALWAYS use while: guard against spurious wakeups\n        pthread_cond_wait(&amp;cv, &amp;m); // releases m while waiting, reacquires before return\n    /* proceed */\n    pthread_mutex_unlock(&amp;m);\n}\n\nvoid signal_ready() {\n    pthread_mutex_lock(&amp;m);\n    ready = 1;\n    pthread_cond_signal(&amp;cv);\n    pthread_mutex_unlock(&amp;m);\n}\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#4-semaphores","title":"4. Semaphores","text":"<p>Counting semaphore: integer value with two atomic ops.</p> <ul> <li>P / wait / down() \u2014 decrement (if negative \u2192 block)</li> <li>V / signal / up() \u2014 increment (if \u2264 0 \u2192 wake one)</li> </ul>"},{"location":"os/lec3_ipc_synchronization/#41-binary-semaphore-as-mutex","title":"4.1 Binary Semaphore as Mutex","text":"<pre><code>sem_t s; sem_init(&amp;s, 0, 1);\nsem_wait(&amp;s);   /* CS */\nsem_post(&amp;s);\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#42-producerconsumer-bounded-buffer","title":"4.2 Producer\u2013Consumer (Bounded Buffer)","text":"<pre><code>#define N  ...\nitem_t buf[N]; int in=0, out=0;\n\nsem_t empty, full, mutex;\n\nvoid producer(item_t x){\n    sem_wait(&amp;empty);\n    sem_wait(&amp;mutex);\n    buf[in] = x; in = (in+1)%N;\n    sem_post(&amp;mutex);\n    sem_post(&amp;full);\n}\n\nitem_t consumer(){\n    item_t x;\n    sem_wait(&amp;full);\n    sem_wait(&amp;mutex);\n    x = buf[out]; out = (out+1)%N;\n    sem_post(&amp;mutex);\n    sem_post(&amp;empty);\n    return x;\n}\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#5-monitors","title":"5. Monitors","text":"<ul> <li>High-level abstraction that encapsulates shared data, mutex, and condition variables.</li> <li>Methods are mutually exclusive (wait &amp; signal on CVs inside monitor).</li> </ul> <pre><code>monitor BoundedBuffer {\n  condition not_full, not_empty;\n  item_t buf[N]; int in=0, out=0, cnt=0;\n\n  procedure put(x){\n    while (cnt==N) wait(not_full);\n    buf[in]=x; in=(in+1)%N; cnt++;\n    signal(not_empty);\n  }\n\n  procedure get(out_x){\n    while (cnt==0) wait(not_empty);\n    out_x = buf[out]; out=(out+1)%N; cnt--;\n    signal(not_full);\n  }\n}\n</code></pre>"},{"location":"os/lec3_ipc_synchronization/#6-deadlock","title":"6) Deadlock","text":""},{"location":"os/lec3_ipc_synchronization/#61-coffman-conditions","title":"6.1 Coffman Conditions:","text":"<ol> <li>Mutual Exclusion \u2014 Some resources are non-shareable.</li> <li>Hold and Wait \u2014 A process holds at least one resource and requests additional ones.</li> <li>No Preemption \u2014 Resources cannot be forcibly taken; they must be released voluntarily.</li> <li>Circular Wait \u2014 A circular chain exists: P1 waits for a resource held by P2, \u2026, Pn waits for a resource held by P1.</li> </ol>"},{"location":"os/lec3_ipc_synchronization/#62-prevention","title":"6.2 Prevention","text":"<p>Break at least one Coffman condition:</p> <ul> <li>No Hold-and-Wait: Request all resources at once (downsides: low utilization, possible starvation).</li> <li>Preemption: Allow resources to be preempted and rolled back to a safe state.</li> <li>Resource Ordering: Impose a total order on resource acquisition; processes must request in ascending order.</li> </ul>"},{"location":"os/lec3_ipc_synchronization/#63-avoidance-bankers-algorithm","title":"6.3 Avoidance (Banker\u2019s Algorithm)","text":"<ul> <li>Admit a request only if the system remains in a safe state (there exists some order of process completion).</li> <li>Requires processes to declare maximum resource needs; works best with predictable demands.</li> </ul>"},{"location":"os/lec3_ipc_synchronization/#64-detection-recovery","title":"6.4 Detection &amp; Recovery","text":"<ul> <li>Detection: Build a wait-for graph (WFG); cycle -&gt; deadlock.</li> <li>Recovery: Abort processes and  rollback the victim to a checkpoint and reclaim resources.</li> </ul>"},{"location":"os/lec3_ipc_synchronization/#7-performance-considerations","title":"7) Performance Considerations","text":"Topic Guidance Busy Waiting vs Blocking Spin only for short waits; otherwise block to save CPU. Lock Granularity Coarse locks \u2192 simpler but less parallel; fine-grained \u2192 higher concurrency but complex. Contention Shorten critical sections; partition data; prefer read\u2013write locks for read-mostly workloads. False Sharing Pad hot fields to cache line size to avoid cache thrashing. Lock-Free Use CAS + backoff; manage memory safely (ABA, hazard pointers, epoch reclamation)."},{"location":"os/lec3_ipc_synchronization/#8-memory-ordering","title":"8) Memory Ordering","text":"<ul> <li>Compilers/CPUs may reorder instructions.</li> <li>Locks, atomics, and condition variables establish happens-before relationships.</li> <li>For lock-free algorithms, use C/C++ atomics with explicit memory orders and fences.</li> </ul>"},{"location":"os/lec3_ipc_synchronization/#10-key-terms-to-memorize","title":"10) Key Terms to Memorize","text":"Term Definition Mutual Exclusion Only one thread may execute in the critical section at a time. Progress If CS is free, some waiting thread can proceed. Bounded Waiting Each thread\u2019s wait to enter CS is finitely bounded. Semaphore (P/V) Counting primitive with atomic wait/signal operations. Mutex Binary lock for exclusive access. Condition Variable Wait/signal mechanism for a predicate under a mutex. Monitor Encapsulated shared state + mutex + CVs; methods are mutually exclusive. Deadlock A cycle of threads each waiting for resources held by others. Starvation A thread never progresses despite overall system activity. Spinlock Busy-wait lock using TAS/CAS; best for very short CS."},{"location":"os/lec3_ipc_synchronization/#summary","title":"Summary","text":"<ul> <li>Deadlock requires all four Coffman conditions (prevention breaks at least one)</li> <li>Banker\u2019s Algorithm avoids unsafe states given known maximum demands.</li> <li>Choose primitives and designs that ensure safety (no races) and liveness (no starvation), while optimizing performance.</li> </ul> <p>For deadlock questions: state Coffman, then give one concrete prevention. When using CVs: guard predicates with a while-loop; with semaphores: preserve acquire/release order. Argue both safety and liveness for full credit (what can\u2019t happen vs. what must eventually happen).</p>"},{"location":"os/lec4_scheduling/","title":"Scheduling","text":""},{"location":"os/lec4_scheduling/#schedulers-role","title":"Scheduler\u2019s Role","text":"<ul> <li>Multiple processes competing for CPU time</li> <li>When more than one is Ready, scheduler chooses who runs next</li> <li>Affects perceived system performance</li> <li>Scheduling decisions occur on:</li> <li>Process creation</li> <li>Process exit / blocking</li> <li>I/O completion interrupt</li> </ul>"},{"location":"os/lec4_scheduling/#context-switch-overhead","title":"Context Switch Overhead","text":"<p>Switching tasks is expensive:</p> <ul> <li>Trap to kernel mode</li> <li>Save current CPU state (registers, memory map)</li> <li>Select new process &amp; restore its state</li> <li>Resume execution Too many switches waste CPU time.</li> </ul>"},{"location":"os/lec4_scheduling/#process-cpu-behavior","title":"Process CPU Behavior","text":"<p>Two typical categories:</p> Type Behavior Resource Usage Notes CPU-bound Long computations Mostly CPU Few I/O wait times I/O-bound Short bursts Mostly I/O Spend time waiting <p>Modern systems become more I/O-bound as CPUs get faster.</p>"},{"location":"os/lec4_scheduling/#preemptive-vs-non-preemptive-scheduling","title":"Preemptive vs Non-Preemptive Scheduling","text":"Strategy CPU Ownership Ends When\u2026 Pros Cons Preemptive Quantum expires or higher-priority arrival Better for interactivity More context switch overhead Non-Preemptive Process blocks or exits Simpler, lower overhead Bad response time if long jobs exist <p>Preemption requires timer interrupt/hardware support.</p>"},{"location":"os/lec4_scheduling/#scheduling-goals","title":"Scheduling Goals","text":"System Type Major Metrics All systems Fairness, CPU balance, enforce policy Interactive Response time, proportionality Batch Throughput \u2191, turnaround time \u2193, CPU utilization \u2191 Real-Time Meet deadlines, predictability (NO data loss)"},{"location":"os/lec4_scheduling/#common-scheduling-algorithms","title":"Common Scheduling Algorithms","text":""},{"location":"os/lec4_scheduling/#1-first-come-first-served-fcfs","title":"1. First-Come First-Served (FCFS)","text":"<ul> <li>Non-preemptive</li> <li>Simple FIFO queue  Problem: Long jobs block short ones (convoy effect)</li> </ul>"},{"location":"os/lec4_scheduling/#2-shortest-job-first-sjf","title":"2. Shortest Job First (SJF)","text":"<ul> <li>Non-preemptive, requires job length knowledge</li> <li>Minimizes average turnaround time</li> <li>Not realistic \u2192 runtime unknown Risk: Starvation of long jobs</li> </ul>"},{"location":"os/lec4_scheduling/#3-round-robin-rr","title":"3. Round Robin (RR)","text":"<ul> <li>Preemptive, equal quantum for each process</li> <li>Very common in interactive systems</li> <li>Choice of quantum matters:</li> <li>Too small \u2192 context switching overhead \u2191</li> <li>Too large \u2192 degenerates to FCFS</li> </ul>"},{"location":"os/lec4_scheduling/#4-priority-scheduling","title":"4. Priority Scheduling","text":"<ul> <li>Higher priority tasks run first</li> <li>Can combine with RR per priority class</li> <li>Starvation possible \u2192 fix: aging</li> </ul>"},{"location":"os/lec4_scheduling/#5-lottery-scheduling","title":"5. Lottery Scheduling","text":"<ul> <li>Random scheduling based on # of tickets</li> <li>Dynamic, flexible priority control  </li> <li>Good when fairness with weighted share is desired</li> </ul>"},{"location":"os/lec4_scheduling/#6-earliest-deadline-first-edf","title":"6. Earliest Deadline First (EDF)","text":"<ul> <li>Real-time focused</li> <li>Run task with closest deadline first Hard requirement: process must announce deadline</li> </ul>"},{"location":"os/lec4_scheduling/#policy-vs-mechanism","title":"Policy vs. Mechanism","text":"<ul> <li>Scheduling mechanism: how scheduling is done</li> <li>Scheduling policy: who should run + priority rules</li> <li>Parent process can set children priority parameters \u2192 avoids assumptions  </li> </ul>"},{"location":"os/lec4_scheduling/#scheduling-threads","title":"Scheduling Threads","text":"<p>User-level vs Kernel-level scheduling  </p> <ul> <li>User level threads: Kernel unaware \u2192 user scheduler only</li> <li>Kernellevel threads: Kernel actively chooses runnable threads</li> <li>Determines runnable interleavings (A1, B1, A2, \u2026)</li> </ul>"},{"location":"os/lec4_scheduling/#classic-synchronization-problem-dining-philosophers","title":"Classic Synchronization Problem: Dining Philosophers","text":"<p>Illustrates deadlock + starvation risks</p> <ul> <li>Five philosophers share chopsticks</li> <li>Need mutual exclusion on chopsticks</li> <li>Fix involves:</li> <li>Correct resource ordering</li> <li>Mutex + state tracking</li> <li>Semaphores controlling neighbors  </li> </ul>"},{"location":"os/os_overview/","title":"Operating Systems Overview","text":""},{"location":"os/os_overview/#1-operating-system","title":"1) Operating System","text":"<p>An Operating System (OS) is a program that manages computer hardware and provides services for application programs.  </p>"},{"location":"os/os_overview/#main-roles","title":"Main Roles","text":"<ol> <li>Resource Manager \u2013 Manages CPU, memory, I/O devices, and storage.  </li> <li>Control Program \u2013 Controls execution of user programs and prevents errors.  </li> <li>Abstraction Layer \u2013 Hides hardware complexity and provides simple interfaces.  </li> </ol> <p>OS acts as an intermediary between hardware and user programs</p>"},{"location":"os/os_overview/#2-os-goals-and-functions","title":"2) OS Goals and Functions","text":"Function Key Term Description Resource Allocation CPU Scheduling, Memory Management Allocates system resources among multiple processes Concurrency Control Process Synchronization Prevents data corruption in concurrent execution Protection &amp; Security Access Control, Authentication Manages permissions and secure access Abstraction Virtual Memory, File System Simplifies hardware details via logical models Error Handling Fault Tolerance Detects and recovers from errors"},{"location":"os/os_overview/#3-os-structure-components","title":"3) OS Structure &amp; Components","text":""},{"location":"os/os_overview/#major-components","title":"Major Components","text":"Component Term Core Function Process Manager Process Management Process creation, scheduling, termination Memory Manager Memory Management Controls physical and virtual memory File System File Management Handles files, directories, and permissions I/O Manager Input/Output Management Manages device drivers and interrupts Protection System Security/Protection Provides access control and isolation Command Interpreter Shell Converts user commands to system calls"},{"location":"os/os_overview/#4-os-as-an-abstraction-layer","title":"4) OS as an Abstraction Layer","text":""},{"location":"os/os_overview/#concept","title":"Concept","text":"<p>The OS hides hardware complexity through abstraction layers that make resources easier to use.</p> <p>Flow: Physical Hardware \u2192 OS Abstractions \u2192 System Calls \u2192 Applications</p>"},{"location":"os/os_overview/#example","title":"Example","text":"Hardware OS Abstraction User Interface Disk File File System Memory Address Space Virtual Memory CPU Process Scheduler"},{"location":"os/os_overview/#5-system-calls","title":"5) System Calls","text":""},{"location":"os/os_overview/#definition","title":"Definition","text":"<p>A System Call is the interface between a user program and the **OS kernel, allowing programs to request OS services.</p>"},{"location":"os/os_overview/#categories-of-system-calls","title":"Categories of System Calls","text":"Category Example Description Process Control fork(), exec(), wait() Create, manage, and terminate processes File Manipulation open(), read(), write() Access and modify files Device Manipulation ioctl(), read() Control I/O devices Information Maintenance getpid(), alarm() Retrieve process or system information Communication pipe(), shmget() Enable Inter-Process Communication (IPC)"},{"location":"os/os_overview/#6-computer-system-organization","title":"6) Computer System Organization","text":"<ul> <li>System Layers</li> <li>Hardware \u2013 CPU, Memory, I/O Devices</li> <li>Operating System Kernel \u2013 Core system control</li> <li>System Programs \u2013 Command interpreters, compilers, utilities</li> <li> <p>User Applications \u2013 User-level programs</p> </li> <li> <p>Interrupts An Interrupt is a signal from hardware or software to the CPU indicating an event that needs immediate attention.</p> </li> </ul> <p>The OS maintains an Interrupt Vector Table (IVT) to dispatch appropriate routines.</p>"},{"location":"os/os_overview/#7-modes-of-operation","title":"7) Modes of Operation","text":"Mode Description Example User Mode Restricted access; system calls required for I/O Running user applications Kernel Mode Full access to hardware and system resources Executing system calls, I/O handling <p>Mode Bit: A CPU flag indicating current privilege level.</p> <ul> <li>0 : Kernel Mode</li> <li>1 : User Mode</li> </ul>"},{"location":"os/os_overview/#8-dual-mode-operation-and-protection","title":"8) Dual-Mode Operation and Protection","text":"<p>** Goal: Prevent user programs from directly interfering with OS or other programs **</p>"},{"location":"os/os_overview/#protection-mechanisms","title":"Protection Mechanisms","text":"<ul> <li> <p>Timer Interrupts \u2013 Prevent infinite loops or CPU hogging</p> </li> <li> <p>Memory Protection \u2013 Protect OS and other processes\u2019 memory regions</p> </li> <li> <p>I/O Privilege Levels \u2013 Allow only kernel code to execute device operations</p> </li> </ul>"},{"location":"os/os_overview/#9-boot-process","title":"9) Boot Process","text":"<ol> <li>Power On \u2013 CPU starts execution at a fixed memory address</li> <li>Bootstrap Program (Firmware / BIOS) executes from ROM</li> <li>Bootloader loads the OS kernel into memory</li> <li>Kernel Initialization \u2013 Sets up system tables and starts services</li> <li>User Processes \u2013 Launch user-level programs</li> </ol>"},{"location":"os/os_overview/#10-important-terms-to-memorize","title":"10) Important Terms to Memorize","text":"Term Definition Kernel Core component of OS responsible for hardware management System Call Interface for user programs to request OS services Interrupt Signal notifying CPU of an event needing attention Context Switch CPU switches from one process to another Trap Software-generated interrupt to invoke OS functions Privilege Level CPU protection mode controlling access rights Bootstrap Program Initial firmware code that loads the OS kernel"},{"location":"portfolio/","title":"Portfolio","text":"<p>Portfolio</p> <ul> <li> <p>Cercare-Medical ML Project</p> </li> <li> <p>Anti-Drone Project</p> </li> <li> <p>Remote Feeling Mimicking Chair</p> </li> </ul> <p>OS</p> <ul> <li>Critical Section</li> </ul> <p>Bayesian</p> <ul> <li> <p>Foundations and motivation</p> </li> <li> <p>Review of probability distributions and MLE</p> </li> <li> <p>Prior distributions and Bayesian logic</p> </li> <li> <p>Types and properties of priors</p> </li> <li> <p>Posterior inference mechanics</p> </li> <li> <p>Interval estimation and prediction</p> </li> <li> <p>Hypothesis testing and model comparison</p> </li> <li> <p>Loss functions and optimal decisions</p> </li> <li> <p>Computational methods for Bayesian</p> </li> </ul>"}]}
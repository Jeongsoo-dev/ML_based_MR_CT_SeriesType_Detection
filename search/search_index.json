{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cercare-Medical ML Project","text":"Machine Learning Based Advanced MR and CT Series Type Detection Jeongsoo Pang     Cercare-Medical R&amp;D     ML-Specialist     2024.06.01 - 2024.12.01"},{"location":"#abstract","title":"Abstract","text":"<p>Radiology workflows depend on correctly identifying series types (e.g., MR: DWI, SWI, T1, T2 FLAIR; CT: Angio/Perfusion/Noncontrast) before reconstruction, analysis, or visualization. Vendor-specific DICOM conventions, private tags, nested data, multilingual fields, and missing metadata make rule-based detectors unreliable. This project delivers a production-ready ML pipeline that automatically classifies 8 MR and 3 CT series using only DICOM header metadata. </p> <p>It features: 1. A robust feature-extraction module handling private/nested tags and multilingual headers. 2. Two HistGradientBoosting (HGBC) models\u2014trained with and without <code>SeriesDescription</code>\u2014to remain robust when textual labels are missing or inconsistent. 3. A self-inspection mechanism that flags low-confidence predictions to radiologists for review.</p> <p>Externally validated on partner-hospital datasets, the model achieved 96.69% MR and 99.25% CT accuracy, replacing the legacy C++ detector in production. The design emphasizes maintainability, future retraining, and clinical safety.</p>"},{"location":"#project-goal","title":"Project Goal","text":"<ul> <li>Build an ML model to classify 8 MR and 3 CT series, replacing the company\u2019s rule-based detector.  </li> <li>Ensure the model is easy to retrain for new series and safe to deploy through confidence-based self-inspection.  </li> </ul>"},{"location":"#my-contributions","title":"My Contributions","text":"<ul> <li>Engineered DICOM Header Extractor</li> <li>Data De-biasing: one representative DICOM per 3D study.  </li> <li>Feature Preprocessing for numeric + categorical + missing/string values.  </li> <li>Dual-Model Training: HGBC with/without <code>SeriesDescription</code>.  </li> <li>Self-Inspection Gate with confidence thresholds and top-2 margin.  </li> <li>External Validation &amp; Deployment with hospitals; production replacement.  </li> <li>Explainability with SHAP; reproducible JSON/serialized pipelines.</li> </ul>"},{"location":"#dataset-summary","title":"Dataset Summary","text":"Modality Train Test MR 171 185 CT 271 407 <p>MR (8): <code>pwi_dsc</code>, <code>pwi_dce</code>, <code>swi</code>, <code>dwi</code>, <code>t2</code>, <code>t2_flair</code>, <code>t1</code>, <code>t1_contrast</code> CT (3): <code>ct_angiography</code>, <code>ct_perfusion</code>, <code>ct_noncontrast</code></p>"},{"location":"#feature-overview","title":"Feature Overview","text":"<p>MR: <code>NumberTemporalPositions</code>, <code>PhaseEncodingDirection</code>, <code>RepetitionTime</code>, <code>FlipAngle</code>, <code>InversionTime</code>, <code>EchoTrainLength</code>, <code>MagneticFieldStrength</code>, <code>EchoSpacing</code>, <code>PulseSequenceName</code>, <code>SequenceVariant</code>, <code>Bvalue</code>, <code>ScanOptions</code> </p> <p>CT: <code>ContrastBolusAgent</code>, <code>ExposureTime</code>, <code>KVP</code>, <code>ScanOptions</code>, <code>ReconstructionDiameter</code>, <code>ConvolutionKernel</code>, <code>TableSpeed</code>, <code>SeriesTime</code>, <code>Modality</code></p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<ol> <li>Ingestion: select one DICOM per 3D series from Blackbox server.  </li> <li>Feature Extraction \u2192 normalized, grouped JSON.  </li> <li>Preprocessing: imputation + one-hot (unknown-safe).  </li> <li>Training (HGBC): tuned <code>max_iter=100</code>, <code>lr=0.1</code>, <code>max_leaf_nodes=31</code>, <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>.  </li> <li>Selective Prediction: abstain on low confidence or tight top-2.  </li> <li>Validation/Deployment: external datasets; production replacement.</li> </ol>"},{"location":"#training-hyperparameter-tuning","title":"Training &amp; Hyperparameter Tuning","text":"<p>I treated tuning as an engineering task, not guesswork.</p> <p>Search space (HGBC): - <code>learning_rate \u2208 {0.03, 0.05, 0.07, 0.1}</code> - <code>max_iter \u2208 {200, 400, 800}</code> (with early stopping) - <code>max_leaf_nodes \u2208 {15, 31, 63}</code> - <code>min_samples_leaf \u2208 {10, 20, 40}</code> - <code>l2_regularization \u2208 {0.0, 0.01, 0.05, 0.1}</code> - <code>early_stopping='auto'</code>, <code>validation_fraction=0.1</code>, <code>n_iter_no_change=20</code></p> <p>Protocol: 1. Stratified 5-fold CV on training (patient-level split) to avoid leakage. 2. Random search (200 trials) \u2192 Bayesian refinement (20 trials) on top 10% configs. 3. Class-imbalance control: per-class weighting from inverse frequency; verified no single class dominated loss. 4. Feature pipelines locked (scalers/encoders fit only on train folds) to guarantee reproducibility. 5. Model selection objective: macro-F1 with a tie-breaker on AUROC and coverage at the selective-prediction threshold.</p> <p>Best config (typical): HGBC( learning_rate=0.07, max_iter=400, max_leaf_nodes=31, min_samples_leaf=20, l2_regularization=0.05, early_stopping='auto', validation_fraction=0.1 )</p> <p>Why not plain GBC? On the same folds, plain GBC matched accuracy only when much deeper trees were allowed\u2014training was 3-6\u00d7 slower and variance across folds was higher. With HGBC, histogram binning plus <code>min_samples_leaf</code> gave smoother loss curves and earlier stopping without sacrificing recall on minority classes.</p>"},{"location":"#model-choice-rationale-histgradientboosting-hgbc","title":"Model Choice &amp; Rationale \u2014 HistGradientBoosting (HGBC)","text":"<p>I compared tree-based learners (RandomForest, GradientBoostingClassifier), linear baselines, and HGBC. HGBC won for this use-case:</p> Criterion HGBC (Chosen) Plain GBC Why it matters for DICOM-header metadata Training speed on medium/large tabular data Histogram binning (fast) Exact splits (slow) Faster iteration for tuning/validation on hospital-scale datasets Native handling of missing values Yes Partial/No Robust to sparsity and vendor-specific header gaps Early stopping &amp; validation split Built-in Manual Safe convergence + automatic regularization Regularization knobs <code>l2_regularization</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code> Fewer stable knobs Tighter control \u2192 less overfit on small classes Interpretability Tree-based; SHAP works well Same Feature attributions for clinical QA"},{"location":"#explainability-robustness-model-safety","title":"Explainability, Robustness &amp; Model Safety","text":"<ul> <li>SHAP-based attributions shipped with predictions for audit-readiness; top contributors were TR/TE/FA and sequence-family tags, matching domain intuition.</li> <li>Counterfactual probes: perturbed non-causal strings in textual headers to ensure predictions stayed stable; drift alarms if contribution of text fields spikes.</li> <li>Selective-prediction policy: abstain when (1) max prob &lt; \u03c4\u2081 or (2) top-2 prob gap &lt; \u03c4\u2082; thresholds chosen on validation for F1@coverage.</li> <li>Calibration: isotonic mapping per fold; stored along with the model for consistent probability semantics.</li> <li>Data privacy &amp; governance: PHI removed upstream; experiments run on anonymized headers only; reproducible artifact hashes tracked.</li> </ul>"},{"location":"#deployment-reproducibility","title":"Deployment &amp; Reproducibility","text":"<ul> <li>Single Sklearn Pipeline: <code>preprocess \u2192 model \u2192 calibration \u2192 selective gate</code>; versioned with semantic tags.</li> <li>Determinism: fixed RNG seeds, pinned package versions, and input schema checks (pydantic) at load time.</li> <li>Experiment tracking: run metadata (params, metrics, SHAP summaries, data snapshot hash) logged for every training job.</li> <li>CI checks: unit tests for feature extractors; regression tests to ensure no drift in per-class recall.</li> <li>Monitoring: in production, log coverage/abstention rate and top-k feature drifts; alerts when coverage &lt; 95% or class recall falls &gt; 3pp.</li> </ul>"},{"location":"#evaluation-protocol-metrics","title":"Evaluation Protocol &amp; Metrics","text":"<ul> <li>Splits: patient-level train/val/test; external partner hospitals held-out for final reporting.</li> <li>Primary metrics: macro-F1 (class balance), per-class recall (clinical safety), and overall accuracy.</li> <li>Selective prediction: tuned a probability-margin gate to maximize F1 @ \u226595% coverage; abstentions trigger radiologist review.</li> <li>Calibration: verified reliability via isotonic calibration on validation folds; ECE &lt; 3% on test.</li> <li>Ablations: </li> <li>Text present vs missing <code>SeriesDescription</code> (two-model strategy)  </li> <li>Remove top-k features (stability check)  </li> <li>Swap HGBC\u2192GBC/RandomForest (model choice justification)</li> </ul>"},{"location":"#results-summary","title":"Results Summary","text":"<p>External partner-hospital validation: MR 96.69%, CT 99.25%. Deployed to production; supports safe retraining and human-in-the-loop.</p>"},{"location":"#limitations-next-steps","title":"Limitations &amp; Next Steps","text":"<ul> <li>Cross-vendor generalization: performance is strong but varies on rare protocol variants; plan targeted augmentation and vendor-specific priors.</li> <li>Long-tail classes: continue collecting underrepresented sequences; consider focal loss proxy via class weights and threshold per class.</li> <li>Lightweight text normalization: subword normalization for multilingual <code>SeriesDescription</code> without relying on full NLP stacks.</li> <li>Automated drift triggers: schedule retrain when coverage dips, calibration ECE rises, or SHAP distributions drift beyond control limits.</li> </ul>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was conducted under Cercare-Medical, Denmark (2024) with direct collaboration with the Lead AI Developer, Senior Software Developers, and Operation Team, resulting in a successful production deployment and recommendation Letter from the CTO.</p> <p> </p>"},{"location":"anti_drone/","title":"Anti-Drone Project","text":"Anti-Drone Project \u2014 FMCW Radar &amp; Electro-Optical Fusion Jeongsoo Pang     AI Capacity Competition by Korean National Defense     FMCW Radar Signal &amp; Image Intelligence     2023.11"},{"location":"anti_drone/#abstract","title":"Abstract","text":"<p>The Anti-Drone Project focused on developing a reliable, low-latency machine learning pipeline to detect and classify UAVs using FMCW radar spectrograms and RCS imagery. The system integrates classical machine learning (SVM, Random Forest, Gradient Boosting) and deep convolutional architectures (AlexNet, ResNet, GoogLeNet, NasNet, SqueezeNet) to achieve optimal trade-offs between accuracy, robustness, and real-time inference on edge devices.</p>"},{"location":"anti_drone/#project-objective","title":"Project Objective","text":"<ul> <li>Build an end-to-end ML framework for drone detection and classification from Doppler and RCS data.</li> <li>Benchmark traditional classifiers versus deep CNN backbones for FMCW spectrograms.</li> <li>Evaluate robustness under noise, latency, and hardware constraints for embedded radar platforms.</li> <li>Provide a research-grade reproducible implementation with clear documentation and modularity.</li> </ul>"},{"location":"anti_drone/#dataset-preprocessing","title":"Dataset &amp; Preprocessing","text":"Dataset Description Modality Goorm-AI-04 Drone Doppler FMCW radar Doppler spectrograms labeled by drone type FMCW Spectrogram Goorm-AI-04 RCS Image Radar cross-section images of drone surfaces RCS Imagery Real Doppler RAD-DAR (Kaggle) Real Doppler datasets recorded by the RAD-DAR radar system, a widely used platform internationally FMCW Doppler Drone Remote Controller RF Signal (IEEE Dataport) RF baseband captures from drone remote controllers for auxiliary signal analysis RF / I/Q <ul> <li>Flattening &amp; Normalization: Converted radar tensors to 224 \u00d7 224 gray-scale arrays, normalized with ImageNet statistics for transfer learning compatibility.  </li> <li>Noise Augmentation: Simulated Gaussian noise at \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} to evaluate noise tolerance of SVC, HGBC, and RF models.  </li> <li>Stratified Splits: Ensured balanced representation across drone types with 10% validation sets.  </li> <li>Dynamic Range Calibration: Capped and floor-normalized pixel intensities to mitigate power spikes and saturation artifacts.</li> </ul>"},{"location":"anti_drone/#model-architectures","title":"Model Architectures","text":""},{"location":"anti_drone/#classical-ml-models","title":"Classical ML Models","text":"Model Core Idea Strength LinearSVC / SVC Multi-class margin-maximization on flattened radar frames Fast, interpretable Random Forest Classifier Ensemble of decision trees with bagging Noise-robust HistGradientBoosting Classifier Histogram-based boosting with native categorical support High accuracy / structured data SGDClassifier Online linear optimization baseline Lightweight reference"},{"location":"anti_drone/#deep-cnn-backbones","title":"Deep CNN Backbones","text":"Model Parameter (M) Notes AlexNet 61.0 Classic CNN baseline for radar texture learning GoogLeNet 6.8 Inception-based multi-scale spatial features ResNet-34 / ResNet-101 21.3 / 44.5 Residual skip-connections for stable training SqueezeNet 1.2 Lightweight model ideal for embedded inference NasNet 5.3 Neural architecture search optimized backbone MobileNetV2 3.5 Depthwise separable convs; strong accuracy/FLOPs ratio <p>All deep networks were fine-tuned from PyTorch ImageNet weights, with a custom three-class output layer corresponding to drone categories (quadrotor, fixed-wing, multi-rotor).</p>"},{"location":"anti_drone/#training-strategy","title":"Training Strategy","text":"<ul> <li>Framework: PyTorch + (optional) Hugging Face + W&amp;B logging.  </li> <li>Batch Size: 128 (train) / 20 (eval).  </li> <li>Epochs: 8\u201312 with early stopping and cosine LR scheduler.  </li> <li>Optimizer: AdamW (lr \u2208 {1e-4, 5e-4, 1e-3, 1e-2}, weight decay = 1e-3).  </li> <li>Mixed Precision (FP16): Enabled where supported for throughput.  </li> <li>Metric Callback: Custom <code>compute_metrics()</code> tracking accuracy, F1, precision, recall, and AUC.  </li> <li>Hyperparameter Search: Grid over learning rates and batch sizes + W&amp;B sweeps for convergence profiling.</li> </ul>"},{"location":"anti_drone/#evaluation-methodology","title":"Evaluation Methodology","text":"<ul> <li>Cross-Validation: 10% validation per seed \u2208 {21, 42, 77}.  </li> <li>Noise Perturbation Tests: Measured model accuracy across \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2} noise levels.  </li> <li>Inference Profiling: Averaged 100 runs to estimate per-image latency (<code>perf_counter()</code> loop).  </li> <li>Metrics: Micro-averaged F1, precision/recall, and timing std (\u03bc \u00b1 \u03c3).</li> </ul>"},{"location":"anti_drone/#results-summary","title":"Results Summary","text":"Model Accuracy (%) F1 Avg Inference (s) Params (M) Linear SVC 92.4 0.92 0.004 \u2013 Hist GB Classifier 95.1 0.95 0.007 \u2013 Random Forest 94.8 0.94 0.009 \u2013 SqueezeNet 97.3 0.97 0.012 1.2 ResNet-34 96.5 0.96 0.017 21.3 GoogLeNet 96.9 0.96 0.015 6.8 <p>SqueezeNet achieved the best accuracy-efficiency balance, making it the production-ready model for real-time radar deployment.</p>"},{"location":"anti_drone/#additional-experiment-noise-on-image-datasets","title":"Additional Experiment \u2014 Noise on Image Datasets","text":"<p>To stress-test robustness, we performed accuracy experiments on noise-added image datasets (Gaussian noise with \u03c3\u00b2 \u2208 {1e-4 \u2026 1e-2}). We evaluated both CNNs and classical models:</p> <ul> <li>CNNs: ResNet-101, GoogLeNet, MobileNetV2 </li> <li>Classical: SVC, Linear SVC, Gradient Boosted Decision Trees (HistGBDT), Random Forest</li> </ul> <p>The plot below summarizes accuracy by model across noise levels:</p> <p></p> <p>Key takeaways: - MobileNetV2 and GoogLeNet maintained high accuracy up to moderate noise due to depthwise separable/inception multi-scale features. - ResNet-101 remained the most stable at higher noise regimes, likely due to deeper residual capacity and batch-norm smoothing. - Among classical models, HistGBDT outperformed RF and SVC under heavier noise, aligning with its histogram-binning robustness.</p>"},{"location":"anti_drone/#technical-highlights","title":"Technical Highlights","text":"<ul> <li>Hybrid Experimentation Framework: Unified classical + deep models within a single harness for direct benchmarking.  </li> <li>Custom Collate Fn: Converted raw radar tensors into 3-channel images via NumPy + PIL augmentation before Torch batching.  </li> <li>W&amp;B Integration: Auto-logged metrics, confusion matrices, parameter sweeps, and artifacts with reproducible seeds.  </li> <li>Cross-Domain Validation: Consistent &gt;96% accuracy on unseen test and noise-augmented datasets.  </li> <li>Explainability: Interpreted radar spectrogram activations using Grad-CAM to validate model attention on micro-Doppler regions.</li> </ul>"},{"location":"anti_drone/#deployment-considerations","title":"Deployment Considerations","text":"<ul> <li>Edge Inference Optimization: Quantized SqueezeNet to FP16, achieving &gt;80 FPS on Jetson Nano.  </li> <li>Model Serialization: Exported via TorchScript for embedded radar pipeline.  </li> <li>Noise-Aware Retraining: Integrated Gaussian perturbation generator for continual retraining in field conditions.</li> </ul>"},{"location":"anti_drone/#future-work","title":"Future Work","text":"<ul> <li>Real-time fusion with EO/IR imagery via feature-level concatenation.  </li> <li>Temporal smoothing (LSTM / Transformer) for tracking moving drones.  </li> <li>Radar signature augmentation using simulated micro-Doppler patterns.</li> </ul>"},{"location":"anti_drone/#repository-structure","title":"Repository Structure","text":"<p>Link to Repository: https://github.com/jpangece/Anti_Drone_System</p> File Description <code>AlexNet.py</code>, <code>GoogLeNet.py</code>, <code>ResNet34.py</code>, <code>ResNet101.py</code>, <code>SqueezeNet.py</code>, <code>NasNet.py</code> Deep CNN backbones (TorchVision compatible) <code>RandomForest.py</code>, <code>SVM_SB_RFC.py</code> Classical ML baselines (SVM, RF, HistGBDT) <code>README.md</code> Documentation and experimental setup <code>wandb/</code> Training logs, sweeps, and performance dashboards"},{"location":"anti_drone/#acknowledgment","title":"Acknowledgment","text":"<p>This work was part of a collaborative defense-AI initiative, integrating FMCW radar and EO sensor data for UAV detection. All experiments were executed using PyTorch 2.0 and W&amp;B, with full experiment reproducibility ensured via deterministic seeds and dataset versioning.</p>"},{"location":"remote_chair/","title":"Remote Feeling Mimicking Chair","text":"Remote Feeling Mimicking Chair \u2014 Low-Latency Dual-Chair Haptic Teleoperation Jeongsoo Pang     UM\u2013SJTU Joint Institute &amp; BuilderX (sponsor)     2025 Design Expo    Abstract <p> This project presents a dual-chair haptic teleoperation system that reproduces both the tilt (pitch/roll) and vibration of a remote operator\u2019s seat in real time. Using a three-actuator Stewart-inspired platform driven by 24 V DC linear actuators, the system achieves a \u00b115\u00b0 motion range and an actuation speed of approximately 84 mm/s. A 6-axis ICM-45686 IMU mounted on the remote chair streams motion data over Bluetooth Low Energy (BLE) with sub-10 ms latency to an ESP32 controller that performs real-time inverse-kinematics control through BTS7960 PWM drivers. A multi-threaded firmware running on FreeRTOS ensures parallel handling of IMU sampling, BLE communication, and actuator feedback, producing smooth motion transitions with negligible delay. Experimental validation at the 2025 SJTU Design Expo confirmed stable operation. </p>"},{"location":"remote_chair/#system-overview","title":"System Overview","text":"<p>The Remote Feeling Mimicking Chair (RFMC) consists of two physically separate yet electronically synchronized platforms:</p> <ul> <li>Chair 1 (Source): Mounted on a moving machine or vehicle; captures inertial data via IMU.  </li> <li>Chair 2 (Replica): Receives data, reconstructs tilt and vibration in real time.</li> </ul> <p>Each chair integrates mechanical, electrical, and firmware subsystems optimized for modular assembly, low cost, and human safety. BLE is used for its ultra-low latency and native multithreading support on ESP32, allowing command rates up to 300 Hz without packet loss. The entire system weighs under 20 kg and can be assembled in less than 45 minutes.</p> Subsystem Key Components Function Sensing ICM-45686 IMU (6-axis) Captures motion and vibration up to 1 kHz Processing ESP32 dual-core MCU Computes inverse kinematics &amp; PID control Transmission BLE GATT protocol Low-latency data relay (&lt; 10 ms) Actuation 3 \u00d7 24 V DC worm-gear linear actuators Generate seat tilt (pitch/roll) Power 24 V 10 A DC supply Shared source for actuators &amp; logic Feedback 5\u201310 Hz vibration motor Simulates terrain resonance"},{"location":"remote_chair/#mechanical-design-and-kinematics","title":"Mechanical Design and Kinematics","text":"<p>The mechanical platform is triangular and symmetric, each actuator mounted at 0\u00b0, 120\u00b0, and 240\u00b0. This configuration balances torque loads, minimizes moment coupling, and reduces the number of control equations from six (in full Stewart systems) to three, maintaining 2-DOF control (pitch, roll) while preserving realism.</p>"},{"location":"remote_chair/#key-structural-highlights","title":"Key Structural Highlights","text":"<ul> <li>Actuator Thrust: 980 N (100 kgf per unit)  </li> <li>Effective Torque: ~12.8 N\u00b7m per actuator at \u00b115\u00b0 tilt  </li> <li>Frame Material: 6061-T6 aluminum profile with 9 mm plywood seat  </li> <li>Bearing Interfaces: M8 rod-end ball joints to absorb lateral shear  </li> <li>Base Geometry: Equilateral triangle, side length 540 mm  </li> <li>Center Height (rest): 230 mm \u2192 variable up to \u00b145 mm during tilt  </li> </ul> <p>Finite Element Analysis (FEA) results show maximum deformation of 0.47 mm at 800 N load, corresponding to a Von Mises stress of 42.3 MPa, well below the aluminum yield strength (\u2248 275 MPa). Safety factor: &gt; 3.1 under full tilt and payload conditions.</p> <p>The inverse kinematics model converts desired Euler angles to actuator lengths via precomputed lookup tables, updated at 200 Hz. This ensures real-time motion synchronization with minimal computational overhead on ESP32.</p>"},{"location":"remote_chair/#control-and-electronics","title":"Control and Electronics","text":""},{"location":"remote_chair/#sensor-sampling","title":"Sensor &amp; Sampling","text":"<p>The ICM-45686 IMU is configured for 1 kHz raw sampling, averaged to 100 Hz for transmission stability. Its digital motion processor (DMP) reduces noise and bias drift using a complementary Kalman filter.</p>"},{"location":"remote_chair/#communication-timing","title":"Communication &amp; Timing","text":"<p>BLE is configured with: - Connection interval: 7.5 ms - MTU size: 247 bytes - Transmission rate: 300 packets/s (orientation + vibration data) Latency tests show mean 7.3 ms delay, 99th percentile &lt; 9.4 ms, even under high interference.</p>"},{"location":"remote_chair/#actuation-feedback","title":"Actuation &amp; Feedback","text":"<ul> <li>BTS7960 Motor Drivers (43 A peak): PWM range 1\u20132 kHz, dual-direction control.  </li> <li>PID loop frequency: 200 Hz; tuned via Ziegler\u2013Nichols method for critical damping (Kp = 2.1, Ki = 0.4, Kd = 0.12).  </li> <li>PWM resolution: 12-bit native hardware control.  </li> <li>Vibration Actuator: Driven by PWM (0\u2013255) mapped to vibration intensity, frequency 5\u201310 Hz (engine resonance band).</li> </ul>"},{"location":"remote_chair/#power-management","title":"Power Management","text":"<p>All systems share a regulated 24 V 10 A DC bus with reverse-polarity protection and EMI filter. Measured steady-state power draw: ~110 W, peak startup: &lt; 160 W. Thermal analysis confirmed continuous operation below 55\u00b0C at full duty.</p>"},{"location":"remote_chair/#firmware-and-software","title":"Firmware and Software","text":"<p>ESP32\u2019s dual-core FreeRTOS design enables fully asynchronous operation:</p> Core Process Description Core 0 BLE stack Handles GATT communication and packet integrity Core 1 Control loop Executes kinematics, PID, and PWM updates"},{"location":"remote_chair/#thread-breakdown","title":"Thread Breakdown","text":"<ul> <li>Task 1: IMU read \u2192 DMP filtering \u2192 queue buffer (1 kHz \u2192 100 Hz)  </li> <li>Task 2: BLE transmit \u2192 checksum validation (every 3 ms)  </li> <li>Task 3: Inverse kinematics + PWM update (5 ms cycle)  </li> <li>Task 4: Vibration motor modulation (adaptive rate 5\u201310 Hz)  </li> </ul> <p>Lookup tables were precomputed for angle-to-stroke mapping, cutting onboard computation time by 60%. BLE retransmission queue ensures 0.00 % packet loss at up to 2.4 GHz channel interference.</p>"},{"location":"remote_chair/#experimental-validation","title":"Experimental Validation","text":"<p>All subsystems underwent systematic testing and calibration.</p> Test Type Metric Measured Result Remarks Tilt accuracy &lt; 0.9\u00b0 avg error \u00b115\u00b0 motion 5\u00b0 step motion, 10 cycles Latency 7.3 ms mean BLE + PWM pipeline &lt; 10 ms end-to-end Vibration Reproduction 5\u201380 Hz range Peak sensitivity 5\u201310 Hz Human resonance frequency Actuator Speed 83.7 mm/s Full stroke 3.7 s Verified via encoder Payload Capacity &gt; 1000 N 3.0\u00d7 safety factor Structural stability Runtime Endurance 90 min Stable temp &lt; 55\u00b0C Full demo Power Draw 110 W avg 24 V supply No overcurrent events <p>Data logging using a 1000 Hz timestamped serial stream confirmed temporal synchronization between source and replica chairs with correlation coefficient r = 0.984 (5 Hz motion cycles).</p>"},{"location":"remote_chair/#results-discussion","title":"Results Discussion","text":"<p>The RFMC system achieved human-perceptible realism with negligible delay and noise-induced jitter. Subjective trials rated feedback realism at 4.6 / 5.0 for tilt response and 4.4 / 5.0 for vibration clarity. Unlike high-cost 6-DOF Stewart platforms (typically &gt; 10 000 USD), the proposed 3-actuator variant achieved equivalent dynamic response using hardware totaling &lt; 400 USD.</p> <p>Comparative benchmarks vs. commercial systems:</p> System DOF Latency (ms) Load (N) Cost (USD) SimCraft APEX 3 3 12\u201315 1300 14 000 D-BOX G5 3 10\u201312 1000 8 000 RFMC (ours) 2 7\u20139 1000 ~400"},{"location":"remote_chair/#applications-and-future-work","title":"Applications and Future Work","text":"<p>Applications - Remote machinery operation (crane, excavator) - Training simulators for heavy-vehicle operators - Rehabilitation chairs for vestibular therapy - Remote telepresence in hazardous environments</p> <p>Future Enhancements 1. Integrate force sensors on actuators for closed-loop bidirectional feedback. 2. Expand motion to 6-DOF by adding heave, surge, yaw axes. 3. Replace BLE with Wi-Fi 6E or private 5G for long-distance telepresence (&gt; 500 m). 4. Incorporate AI-based adaptive control for motion prediction and compensation.  </p>"},{"location":"remote_chair/#acknowledgment","title":"Acknowledgment","text":"<p>Developed by Team 1 (Jeongsoo Pang et al.) Under SJTU UM\u2013JI Capstone Design (2025) and Builder X support, this project demonstrates that haptic telepresence can be achieved using compact mechanical systems and optimized firmware with industry-grade precision.</p>"},{"location":"bayesian/","title":"Bayesian Methods","text":"<p>Short writeups and code for Bayesian inference, conjugate priors, MCMC, and probabilistic programming (PyMC/NumPyro).</p> <ul> <li>Coming soon: Beta\u2013Binomial A/B tests, hierarchical models, HMC diagnostics, and calibration plots.</li> </ul>"},{"location":"bayesian/computation/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/foundations/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/hypothesis-models/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/intervals-prediction/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/loss-decisions/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/mle-review/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/posterior-inference/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/priors-logic/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"bayesian/priors-types/","title":"Page Title","text":"<p>Work-in-progress.</p>"},{"location":"images/","title":"IMAGES","text":""},{"location":"os/","title":"Operating Systems","text":"<p>This section will collect kernels, scheduling, memory, and concurrency notes\u2014plus small labs and demos.</p> <ul> <li>Coming soon: Linux kernel reading notes, lock-free queues, epoll/reactor toy server, NUMA experiments.</li> </ul>"},{"location":"os/critical_section/","title":"OS : Critical Section","text":"<ul> <li>Author: Jeongsoo Pang  </li> </ul>"},{"location":"os/critical_section/#1-what-is-a-critical-section","title":"1) What is a \u201cCritical Section\u201d?","text":"<ul> <li>Critical Section (CS): A section of code that accesses shared state (e.g., shared variables, file descriptors, kernel data structures, device registers) and therefore must not be executed by more than one thread/process at the same time.</li> <li>Goal: Prevent race conditions\u2014nondeterministic bugs caused by interleavings of reads/writes to shared data. Classic symptoms of a race:</li> <li>Lost updates (A writes, then B overwrites).</li> <li>Read of inconsistent/partial state.</li> <li>Occasional test flakiness that \u201cgoes away\u201d when adding prints or sleeps.</li> </ul>"},{"location":"os/critical_section/#2-the-critical-section-problem","title":"2) The \u201cCritical Section Problem\u201d","text":"<p>A correct solution enforces these three properties (originally by Dijkstra):</p> <p>Mutual Exclusion: At most one thread is inside the CS at any time.</p> <p>Progress: If no thread is inside the CS, one of the threads wishing to enter must be able to proceed\u2014no unrelated thread can postpone them forever.</p> <p>Bounded Waiting (No Starvation): There is a finite bound on the number of times other threads can enter their CS after a thread has requested entry and before it gets in.</p> <p>(Performance desiderata, not a safety property): Avoid busy waiting when possible; minimize context switches; scale with cores.</p>"},{"location":"os/critical_section/#3-the-4-part-structure-of-concurrent-code","title":"3) The 4-part Structure of Concurrent Code","text":"<pre><code>Entry Section -&gt; Critical Section -&gt; Exit Section -&gt; Remainder Section\n(check/acquire) (touch shared) (release) (private work)\n</code></pre> <ul> <li>Entry: Acquire the right to enter the CS (lock, protocol).</li> <li>Critical Section: Access/modify shared state.</li> <li>Exit: Release the right (unlock, clear flags).</li> <li>Remainder: Do private or non-shared work.</li> </ul>"},{"location":"os/critical_section/#4-naive-incorrect-attempts","title":"4) Na\u00efve (Incorrect) Attempts","text":"<ul> <li>Disabling interrupts (user space): Not allowed; even in kernel it only prevents preemption on one CPU\u2014does not stop other cores.</li> <li>Simple flags: <code>if (available) available=false;</code> is itself a race.</li> <li>Just sleep/yield: Timing hacks are nondeterministic and brittle.</li> </ul>"},{"location":"os/critical_section/#5-classic-software-only-algorithms-historical-but-educational","title":"5) Classic Software-Only Algorithms (Historical but Educational)","text":"<p>Work on sequential consistency and shared memory. Useful to understand progress/bounded waiting.</p>"},{"location":"os/critical_section/#51-petersons-algorithm-2-threads","title":"5.1 Peterson\u2019s Algorithm (2 threads)","text":"<pre><code>// Shared\nvolatile bool want[2] = {false, false};\nvolatile int turn = 0;\n\n// Thread i (i in {0,1}):\nwant[i] = true;\nturn = 1 - i;\nwhile (want[1 - i] &amp;&amp; turn == 1 - i) {\n/* busy wait /\n}\n/ ---- Critical Section ---- */\nwant[i] = false;\n</code></pre> <ul> <li>Satisfies mutual exclusion, progress, bounded waiting (under SC).</li> <li>Mostly pedagogical; compilers/CPUs reorder \u2192 needs memory barriers in practice.</li> </ul>"},{"location":"os/critical_section/#52-dekker-bakery-lamport-etc","title":"5.2 Dekker, Bakery (Lamport), etc.","text":"<ul> <li>Dekker\u2019s: First correct solution for two processes without atomic ops.</li> <li>Bakery algorithm: N-process generalization (like taking a number at a bakery). Works under SC with read/write variables only.</li> </ul>"},{"location":"os/critical_section/#6-hardware-support-atomic-primitives","title":"6) Hardware Support: Atomic Primitives","text":"<p>Modern solutions rely on atomic read-modify-write (RMW) instructions: - Test-and-Set (TAS):</p> <pre><code>bool test_and_set(bool *x) {\nbool old = *x;\n*x = true;\nreturn old;\n}\n</code></pre> <ul> <li>Compare-and-Swap (CAS): <code>CAS(addr, expected, new)</code> atomically does:</li> </ul> <pre><code>if (*addr == expected) { *addr = new; return true; } else return false;\n</code></pre> <ul> <li>Fetch-and-Add (FAA), XCHG, or LL/SC (Load-Linked/Store-Conditional).</li> </ul> <p>Memory Ordering: Many CPUs are not sequentially consistent. Use: - Acquire on loads that observe a lock; Release on stores that unlock. - Fences/barriers as needed (e.g., <code>atomic_thread_fence(memory_order_seq_cst)</code>). - In C/C++ atomics, pair <code>memory_order_acquire</code> with <code>memory_order_release</code>.</p>"},{"location":"os/critical_section/#7-locks-and-locking-primitives","title":"7) Locks and Locking Primitives","text":""},{"location":"os/critical_section/#71-spinlock-busy-wait","title":"7.1 Spinlock (busy-wait)","text":"<p>Use when: CS is very short and threads are truly running on different CPUs.</p> <pre><code>// TAS spinlock\nstd::atomic&lt;bool&gt; locked{false};\n\nvoid lock() {\nwhile (locked.exchange(true, std::memory_order_acquire)) {\n// spin\n}\n}\n\nvoid unlock() {\nlocked.store(false, std::memory_order_release);\n}\n</code></pre> <ul> <li>Pros: Simple, low latency for tiny CS.</li> <li>Cons: Wastes CPU cycles; terrible if CS may block/sleep or be long. Improvements:</li> <li>Test\u2013Test&amp;Set: Reduce cache traffic by first spinning on reads.</li> <li>Backoff: Exponential backoff to reduce contention.</li> <li>Queue locks (MCS, CLH): Fair spinning; each waiter spins on a local variable, improving scalability under high contention.</li> </ul>"},{"location":"os/critical_section/#72-mutex-sleeping-lock","title":"7.2 Mutex (sleeping lock)","text":"<p>Use when: CS can be longer or a thread may block inside CS. - If lock unavailable, the kernel places thread on a wait queue (no CPU burn). - Often features: fairness, priority inheritance, timed trylock.</p> <pre><code>pthread_mutex_lock(&amp;m);\n/* critical section */\npthread_mutex_unlock(&amp;m);\n</code></pre>"},{"location":"os/critical_section/#73-readerwriter-sharedexclusive-locks","title":"7.3 Reader\u2013Writer (Shared/Exclusive) Locks","text":"<ul> <li>Multiple readers can enter concurrently; writers need exclusivity.</li> <li>Variants: Reader-preferred, Writer-preferred, Fair (avoid starvation).</li> <li>Be careful: Reader preference can starve writers; writer preference can reduce read throughput.</li> </ul>"},{"location":"os/critical_section/#74-recursive-locks","title":"7.4 Recursive locks","text":"<ul> <li>Permit the same thread to lock the same mutex multiple times.</li> <li>Avoid unless necessary (they hide design issues and risk deadlocks).</li> </ul>"},{"location":"os/critical_section/#8-higher-level-constructs","title":"8) Higher-Level Constructs","text":""},{"location":"os/critical_section/#81-semaphores","title":"8.1 Semaphores","text":"<ul> <li>Counting semaphore: integer \u2265 0 with <code>P()/wait()</code> and <code>V()/signal()</code>.</li> <li>Binary semaphore \u2248 mutex (but with different semantics\u2014no ownership).</li> <li>Great for resource counting and producer\u2013consumer.</li> </ul> <pre><code>semaphore empty = N; // free slots\nsemaphore full = 0; // filled slots\nmutex m = 1;\n\nproducer:\nwait(empty);\nwait(m);\nput(item);\nsignal(m);\nsignal(full);\n\nconsumer:\nwait(full);\nwait(m);\nget(item);\nsignal(m);\nsignal(empty);\n</code></pre>"},{"location":"os/critical_section/#82-monitors","title":"8.2 Monitors","text":"<ul> <li>Language-level construct: only one thread executes a monitor\u2019s method at a time (implicit mutual exclusion).</li> <li>Condition variables (CVs) inside a monitor provide waiting and signaling: -\\ <code>wait(cv)</code>: atomically releases the monitor lock and blocks. -\\ <code>signal(cv)</code> or <code>broadcast(cv)</code>: wake one/all waiting threads. Two CV semantics:</li> <li>Hoare-style: Signal hands off immediately to the waiter (rare in practice).</li> <li>Mesa-style (POSIX/C++/Java): Signal puts waiter on ready queue; signaller continues; waiter must recheck the condition after waking. Mesa-style pattern (always use loops):</li> </ul> <pre><code>pthread_mutex_lock(&amp;m);\nwhile (!predicate())\npthread_cond_wait(&amp;cv, &amp;m);\n/* critical section based on predicate holds */\npthread_mutex_unlock(&amp;m);\n</code></pre>"},{"location":"os/critical_section/#9-kernel-vs-user-space","title":"9) Kernel vs. User Space","text":"<ul> <li>User-space: pthread mutexes/CVs often start as futex/adaptive locks; kernel only involved on contention.</li> <li>Kernel-space: Uses a mix of spinlocks (short, IRQ-safe regions), sleeping locks (mutexes/rwsems), and RCU for read-mostly structures.</li> <li>Disabling preemption/interrupts: Kernel may do this for very short non-preemptible CS on a CPU; not a substitute for cross-CPU mutual exclusion.</li> </ul>"},{"location":"os/critical_section/#10-design-patterns-for-critical-sections","title":"10) Design Patterns for Critical Sections","text":""},{"location":"os/critical_section/#101-keep-cs-small-and-fast","title":"10.1 Keep CS Small and Fast","text":"<ul> <li>Move compute-heavy work outside the CS.</li> <li>Copy data in/out if needed, modify locally, then write back inside a short CS.</li> </ul>"},{"location":"os/critical_section/#102-hand-over-hand-lock-coupling","title":"10.2 Hand-over-hand (Lock Coupling)","text":"<ul> <li>For linked data structures (e.g., lists, trees): lock the next node before unlocking the current.</li> </ul>"},{"location":"os/critical_section/#103-partitioningsharding","title":"10.3 Partitioning/Sharding","text":"<ul> <li>Use fine-grained locks per bucket/partition to reduce contention.</li> <li>Or use lock striping\u2014array of locks chosen by hash.</li> </ul>"},{"location":"os/critical_section/#104-read-mostly-data-rcu-read-copy-update","title":"10.4 Read-Mostly Data: RCU (Read-Copy-Update)","text":"<ul> <li>Readers run lock-free, writers create a new version and defer freeing old versions until readers quiesce.</li> </ul>"},{"location":"os/critical_section/#105-lock-free-wait-free","title":"10.5 Lock-free / Wait-free","text":"<ul> <li>Use CAS/LL-SC loops; often faster under contention and avoids deadlock.</li> <li>Requires careful ABA handling (e.g., version tags or hazard pointers).</li> </ul>"},{"location":"os/critical_section/#11-correctness-concerns","title":"11) Correctness Concerns","text":""},{"location":"os/critical_section/#111-deadlock","title":"11.1 Deadlock (\uad50\ucc29\uc0c1\ud0dc)","text":"<p>Conditions (Coffman):</p> <p>Mutual exclusion</p> <p>Hold and wait</p> <p>No preemption</p> <p>Circular wait Avoidance/Prevention: - Impose a global lock ordering; always acquire in the same order. - Try-lock with backoff; detect cycles and release. - Two-phase locking: acquire all needed locks first; if failure, release and retry. - Hierarchical locking: Levels or lock ranks enforced by static analysis.</p>"},{"location":"os/critical_section/#112-starvation","title":"11.2 Starvation","text":"<ul> <li>A thread waits indefinitely while others repeatedly proceed.</li> <li>Use FIFO/queue locks (MCS), fair mutex, or ticket locks.</li> </ul>"},{"location":"os/critical_section/#113-priority-inversion","title":"11.3 Priority Inversion","text":"<ul> <li>Low-priority thread holds a lock; high-priority thread waits while a medium-priority thread runs\u2014inversion.</li> <li>Fix: Priority inheritance (temporarily boosts holder\u2019s priority) or priority ceiling protocols.</li> </ul>"},{"location":"os/critical_section/#114-reentrancy-and-signals","title":"11.4 Reentrancy and Signals","text":"<ul> <li>Don\u2019t take the same non-recursive lock in a signal handler or callback that interrupts the holder \u2192 deadlock.</li> <li>Keep signal handlers reentrant and minimal.</li> </ul>"},{"location":"os/critical_section/#115-memory-ordering","title":"11.5 Memory Ordering","text":"<ul> <li>Always pair acquire on lock with release on unlock so CS writes become visible before unlock and lock sees latest state.</li> </ul>"},{"location":"os/critical_section/#12-performance-considerations","title":"12) Performance Considerations","text":"<ul> <li>Contention: Prefer fine-grained or sharded locks; avoid global bottlenecks.</li> <li>NUMA effects: Avoid bouncing cachelines; use per-CPU/per-NUMA locks or data.</li> <li>Spinning vs sleeping: \\ \\ - Spin for short holds (microseconds, in kernel or when owner is likely running). \\ \\ - Sleep for longer holds (I/O, blocking calls).</li> <li>Fast path vs slow path: \\ \\ - Attempt trylock + work outside CS; fall back to full lock if necessary.</li> </ul>"},{"location":"os/critical_section/#13-worked-examples","title":"13) Worked Examples","text":""},{"location":"os/critical_section/#131-protecting-a-shared-counter","title":"13.1 Protecting a Shared Counter","text":"<p>Incorrect:</p> <pre><code>// race: i++ is read-modify-write\ni++;\n</code></pre> <p>Correct (mutex):</p> <pre><code>pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid increment() {\npthread_mutex_lock(&amp;m);\ni++;\npthread_mutex_unlock(&amp;m);\n}\n</code></pre> <p>Correct (atomic, lock-free):</p> <pre><code>std::atomic&lt;int&gt; i{0};\nvoid increment() {\ni.fetch_add(1, std::memory_order_relaxed);\n}\n</code></pre> <p>If other invariants exist around <code>i</code>, you might need stronger ordering.</p>"},{"location":"os/critical_section/#132-producerconsumer-with-condition-variables-mesa-semantics","title":"13.2 Producer\u2013Consumer with Condition Variables (Mesa semantics)","text":"<pre><code>std::queue&lt;int&gt; q;\nconst size_t CAP = 1024;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t not_full = PTHREAD_COND_INITIALIZER;\npthread_cond_t not_empty = PTHREAD_COND_INITIALIZER;\n\nvoid produce(int item) {\npthread_mutex_lock(&amp;m);\nwhile (q.size() == CAP)\npthread_cond_wait(&amp;not_full, &amp;m);\nq.push(item);\npthread_cond_signal(&amp;not_empty);\npthread_mutex_unlock(&amp;m);\n}\n\nint consume() {\npthread_mutex_lock(&amp;m);\nwhile (q.empty())\npthread_cond_wait(&amp;not_empty, &amp;m);\nint item = q.front(); q.pop();\npthread_cond_signal(&amp;not_full);\npthread_mutex_unlock(&amp;m);\nreturn item;\n}\n</code></pre>"},{"location":"os/critical_section/#133-avoiding-deadlock-with-global-ordering","title":"13.3 Avoiding Deadlock with Global Ordering","text":"<p>Assume two locks <code>A</code> and <code>B</code>. Rule: always acquire in alphabetical order.</p> <pre><code>void f() { // needs A then B\npthread_mutex_lock(&amp;A);\npthread_mutex_lock(&amp;B);\n// CS using A and B\npthread_mutex_unlock(&amp;B);\npthread_mutex_unlock(&amp;A);\n}\n\nvoid g() { // also needs A then B (same order!)\npthread_mutex_lock(&amp;A);\npthread_mutex_lock(&amp;B);\n// CS\npthread_mutex_unlock(&amp;B);\npthread_mutex_unlock(&amp;A);\n}\n</code></pre>"},{"location":"os/critical_section/#14-testing-debugging-concurrency","title":"14) Testing &amp; Debugging Concurrency","text":"<ul> <li>Stress tests: Lots of threads, long runs, randomized yields.</li> <li>Systematic schedulers: Tools that explore interleavings (e.g., model checkers).</li> <li>Thread sanitizers / data race detectors: Instrumented builds to catch races.</li> <li>Assertions &amp; invariants: Check shared state consistency at CS boundaries.</li> <li>Deterministic seeds: Reproduce failures with controlled scheduling when possible.</li> </ul>"},{"location":"os/critical_section/#15-practical-guidelines-checklists","title":"15) Practical Guidelines &amp; Checklists","text":"<ul> <li>Keep CS minimal; do not call unbounded or blocking operations inside.</li> <li>Clearly document lock ownership and lock ordering in comments.</li> <li>Use RAII/scoped guards to avoid forgotten unlocks.</li> <li>Prefer condition variables over ad-hoc sleeps; always <code>while (!cond) wait</code>.</li> <li>Choose the simplest primitive that fits: \\ \\ - short CS &amp; heavy contention \u2192 queue spinlock (MCS) or fine-grained locks \\ \\ - long CS or may block \u2192 mutex/rwlock \\ \\ - resource counting \u2192 semaphore \\ \\ - read-mostly structures \u2192 RCU or rwlock</li> <li>Consider priority inversion in real-time systems; enable priority inheritance.</li> <li>On weakly ordered CPUs, ensure correct acquire/release semantics.</li> </ul>"},{"location":"os/critical_section/#16-common-pitfalls-and-fixes","title":"16) Common Pitfalls (and Fixes)","text":"<ul> <li>Holding a lock while doing I/O \u2192 long hold times, starvation \u2192 Release before I/O or split CS; use reference counts or state flags to keep consistency.</li> <li>Double-checked locking without barriers \u2192 subtle reorder bugs \u2192 use atomics with memory_order or language/library primitives that guarantee safety.</li> <li>Using a semaphore as a mutex \u2192 ownership confusion \u2192 prefer mutex for mutual exclusion; use semaphores for counting resources.</li> <li>Not rechecking predicate after <code>cond_signal</code> \u2192 spurious wakeups \u2192 always loop around the wait.</li> <li>Recursive locks to \u201cfix\u201d deadlock \u2192 masks design issues \u2192 refactor or impose ordering.</li> </ul>"},{"location":"os/critical_section/#17-glossary-quick-reference","title":"17) Glossary (Quick Reference)","text":"<ul> <li>Mutual Exclusion: Only one thread in CS.</li> <li>Progress: If CS empty and someone wants in, someone gets in.</li> <li>Bounded Waiting: Finite bound before a waiter enters.</li> <li>Spinlock: Busy-wait lock using CPU cycles.</li> <li>Mutex: Sleeping lock with ownership semantics.</li> <li>Semaphore: Counting sync primitive (<code>wait/signal</code>).</li> <li>Monitor: Language/runtime construct with implicit mutex + CVs.</li> <li>Condition Variable: Wait/signal on a predicate (with a mutex).</li> <li>CAS/TAS/FAA/LL-SC: Atomic RMW primitives.</li> <li>RCU: Read-Copy-Update; lockless reads.</li> <li>Priority Inversion: Low-prio holds lock; high-prio is blocked by medium-prio.</li> <li>ABA Problem: CAS sees same value <code>A</code> again though it changed (A\u2192B\u2192A).</li> </ul>"},{"location":"os/critical_section/#18-mini-cheat-sheet","title":"18) Mini Cheat-Sheet","text":"<ul> <li>Short CS on multicore? \u2192 Spin (TAS + backoff or MCS).</li> <li>May block / long CS? \u2192 Mutex or RW lock.</li> <li>Multiple readers, rare writers? \u2192 Reader-Writer lock or RCU.</li> <li>Pool of N resources? \u2192 Counting semaphore.</li> <li>Waking sleepers on condition? \u2192 CV with <code>while (!cond) wait</code>.</li> <li>Avoid deadlock \u2192 Global lock order + trylock fallback.</li> </ul>"},{"location":"portfolio/","title":"Portfolio","text":"<p>Portfolio</p> <ul> <li> <p>Cercare-Medical ML Project</p> </li> <li> <p>Anti-Drone Project</p> </li> <li> <p>Remote Feeling Mimicking Chair</p> </li> </ul> <p>OS</p> <ul> <li>Critical Section</li> </ul> <p>Bayesian</p> <ul> <li> <p>Foundations and motivation</p> </li> <li> <p>Review of probability distributions and MLE</p> </li> <li> <p>Prior distributions and Bayesian logic</p> </li> <li> <p>Types and properties of priors</p> </li> <li> <p>Posterior inference mechanics</p> </li> <li> <p>Interval estimation and prediction</p> </li> <li> <p>Hypothesis testing and model comparison</p> </li> <li> <p>Loss functions and optimal decisions</p> </li> <li> <p>Computational methods for Bayesian</p> </li> </ul>"}]}